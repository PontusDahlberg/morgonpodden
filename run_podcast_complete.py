#!/usr/bin/env python3
"""
Komplett podcast-generator f√∂r MMM Senaste Nytt
Med musik-integration och riktig v√§derdata
"""

import os
import sys
import json
import logging
import shutil
import requests
import re
import html
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
from dotenv import load_dotenv

# L√§gg till modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from music_mixer import MusicMixer
from episode_history import EpisodeHistory

# Import Gemini TTS f√∂r f√∂rb√§ttrad dialog
try:
    from gemini_tts_dialog import GeminiTTSDialogGenerator
    GEMINI_TTS_AVAILABLE = True
    GEMINI_TTS_IMPORT_ERROR: Optional[str] = None
except ImportError as e:
    GEMINI_TTS_AVAILABLE = False
    GEMINI_TTS_IMPORT_ERROR = str(e)

# Import KRITISK faktakontroll-agent
try:
    from news_fact_checker import NewsFactChecker
    FACT_CHECKER_AVAILABLE = True
except ImportError as e:
    FACT_CHECKER_AVAILABLE = False

# Import backup faktakontroll (fungerar utan AI)
try:
    from basic_fact_checker import BasicFactChecker, quick_fact_check
    BASIC_FACT_CHECKER_AVAILABLE = True
except ImportError as e:
    BASIC_FACT_CHECKER_AVAILABLE = False

# Import sj√§lvkorrigerande faktakontroll
try:
    from self_correcting_fact_checker import SelfCorrectingFactChecker, auto_correct_podcast_content
    SELF_CORRECTING_AVAILABLE = True
except ImportError as e:
    SELF_CORRECTING_AVAILABLE = False

# Ladda milj√∂variabler
load_dotenv()
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# Konfigurera logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('podcast_generation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

SWEDISH_WEEKDAYS = {
    'Monday': 'm√•ndag',
    'Tuesday': 'tisdag',
    'Wednesday': 'onsdag',
    'Thursday': 'torsdag',
    'Friday': 'fredag',
    'Saturday': 'l√∂rdag',
    'Sunday': 's√∂ndag'
}

SWEDISH_MONTHS = {
    1: 'januari',
    2: 'februari',
    3: 'mars',
    4: 'april',
    5: 'maj',
    6: 'juni',
    7: 'juli',
    8: 'augusti',
    9: 'september',
    10: 'oktober',
    11: 'november',
    12: 'december'
}

# Stopwords f√∂r enkel titel-fingerprinting (f√∂r att undvika dagliga upprepningar)
_TITLE_STOPWORDS = {
    'och', 'eller', 'men', 'att', 'som', 'det', 'den', 'detta', 'dessa', 'en', 'ett', 'i', 'p√•', 'av', 'till',
    'f√∂r', 'med', 'utan', '√∂ver', 'under', 'efter', 'f√∂re', 'om', 'n√§r', 'd√§r', 'h√§r', 'fr√•n', 'mot',
    's√§ger', 'sa', 'uppger', 'enligt', 'nya', 'ny', 'nu', 'idag', 'ig√•r', 'imorgon',
    'the', 'a', 'an', 'and', 'or', 'but', 'to', 'of', 'in', 'on', 'for', 'with', 'from', 'by', 'as', 'at',
}

# Diagnostics
DIAGNOSTICS_FILE = os.getenv('MMM_DIAGNOSTICS_FILE', 'diagnostics.jsonl')
_CURRENT_RUN_ID: Optional[str] = None
_LAST_GEMINI_ERROR: Optional[str] = None


def set_run_id(run_id: str) -> None:
    global _CURRENT_RUN_ID
    _CURRENT_RUN_ID = run_id


def log_diagnostic(event: str, payload: Dict) -> None:
    """Skriv en strukturerad rad till diagnostics.jsonl utan att spamma huvudloggen."""
    try:
        entry = {
            'ts': datetime.now().isoformat(timespec='seconds'),
            'run_id': _CURRENT_RUN_ID,
            'event': event,
            **payload,
        }
        with open(DIAGNOSTICS_FILE, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception:
        # Diagnostik f√•r aldrig stoppa k√∂rningen
        pass


def _set_last_gemini_error(message: Optional[str]) -> None:
    global _LAST_GEMINI_ERROR
    _LAST_GEMINI_ERROR = message


def _canonicalize_url(url: str) -> str:
    """Skapa stabil URL-nyckel som inte p√•verkas av tracking-parametrar."""
    if not url:
        return ""
    try:
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

        parsed = urlparse(url.strip())
        query_pairs = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True)
                       if not k.lower().startswith('utm_') and k.lower() not in {'fbclid', 'gclid'}]
        cleaned = parsed._replace(query=urlencode(query_pairs, doseq=True), fragment="")

        # Normalisera bort trailing slash
        normalized = urlunparse(cleaned).rstrip('/')
        return normalized
    except Exception:
        return url.strip().rstrip('/')


def _title_fingerprint(title: str) -> str:
    """Skapa en grov "fingerprint" av en titel f√∂r dedupe mellan dagar."""
    if not title:
        return ""
    text = title.lower()
    text = re.sub(r'[^\w\s√•√§√∂√Ö√Ñ√ñ-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = [t for t in text.split(' ') if len(t) >= 4 and t not in _TITLE_STOPWORDS and not t.isdigit()]
    if not tokens:
        return ""
    # Sortera och ta en begr√§nsad m√§ngd token f√∂r stabilitet
    tokens = sorted(set(tokens))
    return ' '.join(tokens[:12])


def _is_follow_up_article(title: str, content: str) -> bool:
    """Heuristik: uppf√∂ljningar √§r okej, men daglig repetition ska bort."""
    text = f"{title} {content}".lower()
    # Viktigt: h√•ll detta strikt. M√•nga rubriker inneh√•ller ord som "nu"/"senaste" utan att vara
    # en verklig uppf√∂ljning, vilket annars g√∂r att samma nyheter slinker igenom dag efter dag.
    follow_up_markers = [
        'uppf√∂lj', 'fortsatt', 'ytterligare', 'nya uppgifter', 'uppdater',
        'r√§tteg√•ng', 'dom', 'd√∂md', '√∂verklag', '√•tal', '√•talad', 'f√§ll', 'frias', 'utred',
        'follow-up', 'update', 'new details'
    ]
    return any(m in text for m in follow_up_markers)


def extract_referenced_articles(podcast_content: str, candidate_articles: List[Dict], max_results: int = 8) -> List[Dict]:
    """F√∂rs√∂k avg√∂ra vilka av kandidat-artiklarna som faktiskt refereras i manuset.

    Vi matchar p√• stabila titel-token (inte exakta str√§ngar) f√∂r att undvika att RSS listar k√§llor
    som aldrig n√§mndes i avsnittet.
    """
    if not podcast_content or not candidate_articles:
        return []

    haystack = podcast_content.lower()
    haystack = re.sub(r'[^\w\s√•√§√∂√Ö√Ñ√ñ-]', ' ', haystack)
    haystack = re.sub(r'\s+', ' ', haystack)

    scored: List[tuple[int, Dict]] = []
    for article in candidate_articles:
        title = (article.get('title') or '').strip()
        source = (article.get('source') or '').strip()
        fp = _title_fingerprint(title)
        if not fp:
            continue

        tokens = fp.split(' ')
        token_hits = sum(1 for t in tokens if t in haystack)
        source_hit = 1 if source and source.lower() in haystack else 0
        score = token_hits + source_hit

        # Kr√§ver minst en rimlig tr√§ff f√∂r att r√§knas
        if score >= 2 or (score >= 1 and any(len(t) >= 10 and t in haystack for t in tokens)):
            scored.append((score, article))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [a for _, a in scored[:max_results]]

def get_swedish_weather() -> str:
    """H√§mta aktuell v√§derdata fr√•n SMHI f√∂r svenska landskap"""
    try:
        # Importera SMHI-modulen
        from smhi_weather import SMHIWeatherService
        
        service = SMHIWeatherService()
        weather_summary = service.get_swedish_weather_summary()
        
        logger.info(f"[WEATHER] {weather_summary}")
        return weather_summary
            
    except Exception as e:
        logger.error(f"[WEATHER] Fel vid SMHI v√§der-h√§mtning: {e}")
        # Fallback till wttr.in om SMHI misslyckas
        try:
            logger.info("[WEATHER] Anv√§nder wttr.in som backup")
            # Svenska landskap med representativa st√§der - alla 4 regioner
            regions = [
                ("G√∂taland", "Goteborg"),
                ("Svealand", "Stockholm"),  
                ("S√∂dra Norrland", "Sundsvall"),
                ("Norra Norrland", "Kiruna")
            ]
            
            weather_data = []
            for region_name, api_name in regions:
                try:
                    url = f"https://wttr.in/{api_name}?format=%C+%t"
                    response = requests.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        weather_info = response.text.strip()
                        weather_data.append(f"{region_name}: {weather_info}")
                        
                except Exception:
                    continue
            
            if weather_data:
                weather_text = f"V√§dret idag: {', '.join(weather_data)}"  # Visa alla regioner
                return weather_text
            else:
                return "V√§dret idag: Varierande v√§derf√∂rh√•llanden √∂ver Sverige"
                
        except Exception:
            return "V√§dret idag: Varierande v√§derf√∂rh√•llanden √∂ver Sverige"

def load_config() -> Dict:
    """Ladda konfiguration fr√•n sources.json"""
    try:
        with open('sources.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error("[ERROR] sources.json hittades inte")
        return {}
    except json.JSONDecodeError as e:
        logger.error(f"[ERROR] Fel i sources.json: {e}")
        return {}

def get_openrouter_response(messages: List[Dict], model: str = "openai/gpt-4o-mini") -> str:
    """Skicka f√∂rfr√•gan till OpenRouter API"""
    api_key = os.getenv('OPENROUTER_API_KEY')
    if not api_key:
        raise ValueError("OPENROUTER_API_KEY saknas i milj√∂variabler")
    
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "HTTP-Referer": "https://github.com/PontusDahlberg",
        "X-Title": "MMM Podcast Generator",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 4000
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=90)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']
    except requests.RequestException as e:
        logger.error(f"[ERROR] OpenRouter API error: {e}")
        raise

def generate_structured_podcast_content(weather_info: str, today: Optional[datetime] = None) -> tuple[str, List[Dict]]:
    """Generera strukturerat podcast-inneh√•ll med AI och riktig v√§derdata"""
    
    # Dagens datum f√∂r kontext
    today = today or datetime.now()
    date_str = today.strftime('%Y-%m-%d')
    weekday = today.strftime('%A')
    swedish_weekday = SWEDISH_WEEKDAYS.get(weekday, weekday)
    swedish_month = SWEDISH_MONTHS.get(today.month, today.strftime('%B').lower())
    date_context = f"{swedish_weekday} den {today.day} {swedish_month} {today.year}"
    
    # L√§s tidigare anv√§nda artiklar f√∂r upprepningsfilter (senaste 21 dagarna)
    # (viktigt f√∂r att undvika att samma nyhet tas upp dag efter dag)
    dedupe_days = 21
    used_articles = set()

    # Prim√§rt: anv√§nd en persistent historikfil (fungerar i GitHub Actions via cache)
    history = None
    try:
        from src.news_dedupe import NewsHistory

        history = NewsHistory("news_history.json")
        history.load()
        history.prune(keep_days=60)
        logger.info("[HISTORY] news_history.json loaded (%s keys)", len(history.data.get('items', {})))
    except Exception as e:
        history = None
        logger.warning(f"[HISTORY] Kunde inte initiera news_history.json: {e}")
    try:
        import glob
        article_files = glob.glob('episode_articles_*.json')
        
        # Filtrera p√• datum - bara senaste 21 dagarna
        cutoff_date = datetime.now() - timedelta(days=dedupe_days)
        recent_files = []
        for article_file in article_files:
            try:
                # Extrahera datum fr√•n filnamn: episode_articles_20251014_190405.json
                date_str = article_file.split('_')[2]  # 20251014
                file_date = datetime.strptime(date_str, '%Y%m%d')
                if file_date >= cutoff_date:
                    recent_files.append(article_file)
            except (IndexError, ValueError):
                continue
        
        for article_file in recent_files:
            try:
                with open(article_file, 'r', encoding='utf-8') as f:
                    prev_articles = json.load(f)
                    for article in prev_articles:
                        # L√§gg in b√•de canonical URL och titel-fingerprint f√∂r robust dedupe
                        prev_link = article.get('link', '')
                        prev_title = article.get('title', '')
                        url_key = _canonicalize_url(prev_link)
                        fp_key = _title_fingerprint(prev_title)
                        if url_key:
                            used_articles.add(url_key)
                        if fp_key:
                            used_articles.add(fp_key)
            except Exception as e:
                logger.warning(f"[HISTORY] Kunde inte l√§sa {article_file}: {e}")
        logger.info(f"[HISTORY] Laddade {len(used_articles)} tidigare anv√§nda artiklar f√∂r upprepningsfilter")
    except Exception as e:
        logger.warning(f"[HISTORY] Upprepningsfilter misslyckades: {e}")
    
    # ============================================================
    # MULTI-AGENT NEWS CURATION SYSTEM
    # ============================================================
    logger.info("\n" + "="*80)
    logger.info("ü§ñ AGENT-BASERAD NYHETSKURERING STARTAR")
    logger.info("="*80)
    
    # Importera agent-systemet
    try:
        from news_curation_integration import curate_news_sync
        
        # Anv√§nd agent-systemet f√∂r att kurera artiklar
        available_articles = curate_news_sync('scraped_content.json')
        
        logger.info(f"\n‚úÖ Agent-systemet valde {len(available_articles)} artiklar f√∂r podcast")
        logger.info("="*80 + "\n")
        
    except Exception as e:
        logger.error(f"‚ùå Agent-systemet misslyckades: {e}")
        logger.warning("Faller tillbaka p√• enkel filtrering...")
        
        # FALLBACK: Enkel filtrering om agent-systemet failar
        available_articles: List[Dict] = []
        try:
            with open('scraped_content.json', 'r', encoding='utf-8') as f:
                scraped_data = json.load(f)
                for source_group in scraped_data:
                    source_name = source_group.get('source', 'Ok√§nd')
                    items = source_group.get('items', [])
                    for item in items[:5]:
                        if item.get('link') and item.get('title'):
                            available_articles.append({
                                'source': source_name,
                                'title': item['title'][:100],
                                'content': item.get('content', '')[:300],
                                'link': item['link']
                            })
        except Exception as fallback_error:
            logger.error(f"‚ùå √Ñven fallback-filtrering misslyckades: {fallback_error}")
            available_articles = []
    
    # Skapa artikelreferenser f√∂r AI
    article_refs = ""
    if available_articles:
        # Filtrera bort upprepningar (men till√•t uppf√∂ljningar)
        filtered_articles = []
        skipped_count = 0
        for a in available_articles:
            url_key = _canonicalize_url(a.get('link', ''))
            fp_key = _title_fingerprint(a.get('title', ''))

            # Matcha b√•de "legacy" (episode_articles_*.json) och persistent historik (news_history.json)
            hist_url_key = f"url:{url_key}" if url_key else ""
            hist_fp_key = f"title:{fp_key}" if fp_key else ""

            is_repeat_by_url = bool(url_key and url_key in used_articles)
            is_repeat_by_fp = bool(fp_key and fp_key in used_articles)

            if history is not None:
                try:
                    is_repeat_by_url = is_repeat_by_url or bool(hist_url_key and history.seen_within_days(hist_url_key, dedupe_days))
                    is_repeat_by_fp = is_repeat_by_fp or bool(hist_fp_key and history.seen_within_days(hist_fp_key, dedupe_days))
                except Exception:
                    # Historik ska aldrig stoppa k√∂rningen
                    pass

            is_repeat = is_repeat_by_url or is_repeat_by_fp
            is_follow_up = _is_follow_up_article(a.get('title', ''), a.get('content', ''))

            if is_repeat and not is_follow_up:
                skipped_count += 1
                logger.info(f"[HISTORY] Skipping repeat: {a.get('source', '')} - {a.get('title', '')[:80]}")
                log_diagnostic('article_skipped_repeat', {
                    'source': a.get('source', ''),
                    'title': a.get('title', ''),
                    'link': a.get('link', ''),
                    'repeat_by_url': is_repeat_by_url,
                    'repeat_by_title_fingerprint': is_repeat_by_fp,
                    'url_key': url_key,
                    'title_fingerprint': fp_key,
                })
                continue

            if is_repeat and is_follow_up:
                log_diagnostic('article_allowed_follow_up', {
                    'source': a.get('source', ''),
                    'title': a.get('title', ''),
                    'link': a.get('link', ''),
                    'repeat_by_url': is_repeat_by_url,
                    'repeat_by_title_fingerprint': is_repeat_by_fp,
                })

            filtered_articles.append(a)

            # Markera som sedd i persistent historik n√§r vi v√§ljer att beh√•lla artikeln
            if history is not None:
                try:
                    history.mark_seen([hist_url_key, hist_fp_key])
                except Exception:
                    pass

        if skipped_count:
            logger.info(f"[HISTORY] Filtrerade bort {skipped_count} upprepade artiklar")
            log_diagnostic('dedupe_summary', {
                'skipped_repeat_count': skipped_count,
                'remaining_article_count': len(filtered_articles),
            })

        available_articles = filtered_articles

        # Spara persistent historik (om den √§r aktiverad)
        if history is not None:
            try:
                history.save()
                logger.info("[HISTORY] news_history.json saved")
            except Exception as e:
                logger.warning(f"[HISTORY] Kunde inte spara news_history.json: {e}")

        article_refs = "\n\nTILLG√ÑNGLIGA ARTIKLAR ATT REFERERA TILL:\n"
        for i, article in enumerate(available_articles[:10], 1):
            article_refs += f"{i}. {article['source']}: {article['title']}\n   Inneh√•ll: {article['content']}\n   [Referera som: {article['source']}]\n\n"
    
    prompt = f"""Skapa ett KOMPLETT och DETALJERAT manus f√∂r dagens avsnitt av "MMM Senaste Nytt" - en svensk daglig nyhetspodcast om teknik, AI och klimat.

DATUM (KRITISKT): {date_str} ({date_context})
V√ÑDER: {weather_info}
L√ÑNGD: Absolut m√•l √§r 10 minuter (minst 1800-2200 ord f√∂r talat inneh√•ll)
V√ÑRDAR: Lisa (kvinnlig, professionell men v√§nlig) och Pelle (manlig, nyfiken och engagerad)

{article_refs}

DETALJERAD STRUKTUR:
1. INTRO & V√ÑLKOMST (90-120 sekunder) - Inkludera RIKTIG v√§derinfo fr√•n "{weather_info}"
2. INNEH√ÖLLS√ñVERSIKT (60-90 sekunder) - Detaljerad genomg√•ng av alla √§mnen
3. HUVUDNYHETER (6-7 minuter) - 5-6 nyheter med djup analys och dialog  
4. DJUP DISKUSSION/ANALYS (2-3 minuter) - Lisa och Pelle diskuterar trender och framtid
5. SAMMANFATTNING (60-90 sekunder) - Detaljerad recap av alla √§mnen
6. OUTRO & MMM-KOPPLING (60-90 sekunder) - STARK koppling till huvudpodden "M√§nniska Maskin Milj√∂"

INNEH√ÖLLSKRAV OCH √ÑMNESF√ñRDELNING:
- KRITISKT: Utg√• fr√•n DATUM (KRITISKT) ovan (= dagens datum f√∂r denna k√∂rning). N√§mn inte fel m√•nad (t.ex. "november") eller fel datum.
- Lisa s√§ger "MMM Senaste Nytt" naturligt och professionellt (inte √∂verdrivet)
- Anv√§nd RIKTIG v√§derdata: "{weather_info}" - inte p√•hittade kommentarer om "fin dag i Stockholm"
- Minst 6 konkreta nyheter fr√•n svenska och internationella k√§llor
- OBLIGATORISK √ÑMNESF√ñRDELNING (mycket viktigt f√∂r balans):
  * MINST 50% av nyheterna M√ÖSTE handla om KLIMAT, MILJ√ñ och H√ÖLLBARHET
  * Maximalt 50% f√•r handla om AI och teknologi (inte klimatrelaterad)
  * Prioritera SVENSKA klimat- och milj√∂nyheter n√§r de finns tillg√§ngliga
  * Exempel: Om du har 6 nyheter, MINST 3 ska vara klimat/milj√∂
- Varje nyhet ska vara minst 150-200 ord inklusive diskussion
- Lisa och Pelle ska ha naturliga konversationer med f√∂ljdfr√•gor
- Inkludera siffror, fakta och konkreta exempel
- N√§mn specifika f√∂retag, forskare eller organisationer

REDATIONELL LINJE (KRITISKT):
- MMM Senaste Nytt √§r INTE f√∂r k√§rnkraft som klimatl√∂sning. Presentera inte k√§rnkraft som "l√∂sningen".
- Om k√§rnkraft n√§mns: var saklig och kritisk (mycket h√∂ga kostnader, l√•nga byggtider som f√∂rsenar fossil undantr√§ngning,
  avfallsfr√•gan/slutf√∂rvar i mycket l√•ng tid, och att det riskerar tr√§nga undan snabbare alternativ).

K√ÑLLH√ÑNVISNING - MYCKET VIKTIGT:
- ANV√ÑND ENDAST artiklarna listade ovan som k√§llor f√∂r dina nyheter - ALDRIG sociala medier eller op√•litliga k√§llor
- VARJE nyhet M√ÖSTE baseras p√• en specifik artikel fr√•n seri√∂sa medier (SVT, DN, BBC, Reuters etc.)
- F√ñRBJUDET: Facebook, Twitter/X, Instagram, TikTok, YouTube eller andra sociala medier som k√§llor
- Referera tydligt: "SVT Nyheter rapporterar att...", "BBC News skriver att...", "Reuters meddelar att..."
- Specifika personer M√ÖSTE namnges n√§r de finns i artiklarna (t.ex. "Milj√∂minister Romina Pourmokhtari s√§ger...", "Enligt statsminister Ulf kristersson...")
- Konkreta detaljer M√ÖSTE tas fr√•n artiklarna - p√•hitta ALDRIG fakta
- N√§r m√∂jligt: anv√§nd siffror och fakta fr√•n artiklarna
- Om information saknas i artiklarna - s√§g det tydligt ("detaljerna √§r √§nnu inte k√§nda", "ingen tidsplan har presenterats")
- Undvik vaga termer - var specifik baserat p√• vad som faktiskt st√•r i artiklarna
- Lyssnarna ska kunna f√∂rst√• att nyheten kommer fr√•n en etablerad, trov√§rdig nyhetsk√§lla

OUTRO-KRAV (MYCKET VIKTIGT):
- INGEN teasing av "n√§sta avsnitt" 
- INGA p√•hittade lyssnarfr√•gor
- STARK koppling till huvudpodden "M√§nniska Maskin Milj√∂"
- F√∂rklara att MMM Senaste Nytt √§r en del av M√§nniska Maskin Milj√∂-familjen
- Uppmana lyssnare att kolla in huvudpodden f√∂r djupare analyser
- OBLIGATORISK AI-BRASKLAPP: Lisa och Pelle ska √∂dmjukt f√∂rklara att de √§r AI-r√∂ster och att information kan inneh√•lla fel, h√§nvisa till l√§nkarna i avsnittsinformationen f√∂r verifiering

DIALOGREGLER:
- Anv√§nd naturliga √∂verg√•ngar: "Det p√•minner mig om...", "Aprop√• det...", "Interessant nog..."
- Lisa och Pelle ska ibland avbryta varandra naturligt
- Inkludera korta pauser f√∂r eftertanke: "Hmm, det √§r en bra po√§ng..."
- Anv√§nd svenska uttryck och vardagligt spr√•k
- Variera meningsl√§ngderna f√∂r naturligt flyt
- KRITISKT: Varje replik m√•ste logiskt f√∂lja f√∂reg√•ende - ingen f√•r svara p√• n√•got som inte sagts √§nnu
- KRITISKT: Om n√•gon n√§mner en siffra/statistik, m√•ste den f√∂rst ha presenterats i tidigare replik
- Kontrollera att alla h√§nvisningar ("det", "den siffran", "som du sa") faktiskt refererar till n√•got som redan sagts

FORMAT: Skriv ENDAST som ren dialog med "Talarnamn: Text" - INGEN markdown eller formatering!
F√ñRBJUDET: **, ##, ---, ###, rubriker, markeringar, lyssnarfr√•gor, "n√§sta avsnitt" - bara ren dialog!

EXEMPEL INTRO:
Lisa: Hej och v√§lkommen till MMM Senaste Nytt! Jag heter Lisa.
Pelle: Och jag heter Pelle. Idag √§r det {swedish_weekday} den {today.day} {swedish_month} {today.year}, och {weather_info.lower()}.
Lisa: Ja, det st√§mmer! Men vi har mycket sp√§nnande att prata om idag inom teknik, AI och klimat.

VIKTIGT: Endast dialog - inga rubriker eller formatering! Bara "Namn: Text" rad f√∂r rad.

Skapa nu ett KOMPLETT och L√ÖNGT manus f√∂r dagens avsnitt - kom ih√•g minst 1800 ord:"""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        content = get_openrouter_response(messages)
        logger.info("[AI] Genererade podcast-inneh√•ll med v√§derdata")
        return content, available_articles
    except Exception as e:
        logger.error(f"[ERROR] Kunde inte generera AI-inneh√•ll: {e}")
        # Fallback till mock-inneh√•ll
        return generate_fallback_content(date_str, swedish_weekday, weather_info), available_articles

def generate_fallback_content(date_str: str, weekday: str, weather_info: str) -> str:
    """Fallback-inneh√•ll om AI inte fungerar"""
    try:
        dt = datetime.strptime(date_str, '%Y-%m-%d')
        month_swedish = SWEDISH_MONTHS.get(dt.month, dt.strftime('%B').lower())
        day = dt.day
        year = dt.year
    except Exception:
        month_swedish = datetime.now().strftime('%B').lower()
        day = int(datetime.now().strftime('%d'))
        year = datetime.now().year

    return f"""Lisa: Hej och v√§lkommen till MMM Senaste Nytt! Jag heter Lisa.

Pelle: Och jag heter Pelle. Idag √§r det {weekday} den {day} {month_swedish} {year}, och {weather_info.lower()}.

Lisa: Precis! Vi har mycket att prata om idag inom teknik, AI och klimat.

Pelle: Vi b√∂rjar med att OpenAI har lanserat en f√∂rb√§ttrad version av GPT-4 som de kallar GPT-4 Turbo. Modellen √§r inte bara snabbare, utan ocks√• betydligt billigare att anv√§nda f√∂r utvecklare.

Lisa: Det √§r verkligen stora nyheter, Pelle. Vad tror du det betyder f√∂r svenska f√∂retag som anv√§nder AI?

Pelle: Jag tror det kommer att demokratisera AI-anv√§ndningen. Mindre f√∂retag kommer nu ha r√•d att bygga avancerade AI-l√∂sningar.

Lisa: Sp√§nnande! Vi forts√§tter med klimatnyheter. Tesla har presenterat sina nya 4680-batterier som p√•st√•s ha 50% b√§ttre energidensitet.

Pelle: Det √§r riktigt intressanta nyheter. B√§ttre batterier √§r nyckeln till b√•de elbilar och energilagring.

Lisa: F√∂r att sammanfatta har vi pratat om OpenAI:s nya modell och Teslas batteriteknik.

Pelle: Det h√§r avsnittet av MMM Senaste Nytt √§r en del av v√•rt st√∂rre program M√§nniska Maskin Milj√∂, d√§r vi g√•r djupare in p√• hur teknik, klimat och samh√§lle p√•verkar varandra.

Lisa: Tack f√∂r att ni lyssnade! Vi h√∂rs imorgon med fler nyheter."""

def clean_text_for_tts(text: str) -> str:
    """Ta bort markdown-formattering och andra tecken som inte ska l√§sas upp"""
    # Ta bort markdown-formattering
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # **text** -> text
    text = re.sub(r'\*([^*]+)\*', r'\1', text)      # *text* -> text
    text = re.sub(r'#{1,6}\s*', '', text)           # ### headings -> 
    text = re.sub(r'---+', '', text)                # --- -> 
    text = re.sub(r'^\s*[-*+]\s*', '', text, flags=re.MULTILINE)  # list items
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # [text](link) -> text
    
    # Ta bort andra formattering
    text = re.sub(r'`([^`]+)`', r'\1', text)        # `code` -> code
    text = re.sub(r'_{1,2}([^_]+)_{1,2}', r'\1', text)  # __text__ -> text
    
    # Rensa extra whitespace
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def split_long_text_for_tts(text: str, speaker: str, max_bytes: int = 4000) -> List[Dict]:
    """Dela upp l√•ng text i TTS-kompatibla segment"""
    segments = []
    
    # Om texten √§r kort nog, returnera som ett segment
    if len(text.encode('utf-8')) <= max_bytes:
        return [{'speaker': speaker, 'text': text}]
    
    # Dela vid meningar f√∂rst
    sentences = re.split(r'(?<=[.!?])\s+', text)
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence_bytes = len(sentence.encode('utf-8'))
        
        # Om en enskild mening √§r f√∂r l√•ng, dela den h√•rdare
        if sentence_bytes > max_bytes:
            # Spara nuvarande chunk f√∂rst
            if current_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(current_chunk).strip()
                })
                current_chunk = []
                current_size = 0
            
            # Dela den l√•nga meningen vid komma eller ord
            words = sentence.split()
            word_chunk = []
            word_size = 0
            
            for word in words:
                word_bytes = len((word + ' ').encode('utf-8'))
                if word_size + word_bytes > max_bytes and word_chunk:
                    segments.append({
                        'speaker': speaker,
                        'text': ' '.join(word_chunk).strip()
                    })
                    word_chunk = [word]
                    word_size = word_bytes
                else:
                    word_chunk.append(word)
                    word_size += word_bytes
            
            if word_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(word_chunk).strip()
                })
        else:
            # Kontrollera om vi kan l√§gga till meningen
            if current_size + sentence_bytes > max_bytes and current_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(current_chunk).strip()
                })
                current_chunk = [sentence]
                current_size = sentence_bytes
            else:
                current_chunk.append(sentence)
                current_size += sentence_bytes
    
    # L√§gg till sista chunk
    if current_chunk:
        segments.append({
            'speaker': speaker,
            'text': ' '.join(current_chunk).strip()
        })
    
    return segments

def parse_podcast_text(text: str) -> List[Dict]:
    """Parsa podcast-text i segment med talare och repliker"""
    segments = []
    lines = text.strip().split('\n')
    
    current_speaker = None
    current_text = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Hoppa √∂ver rena formattering-rader
        if line.startswith('#') or line.startswith('---') or line.startswith('### '):
            continue
            
        # Identifiera talare-rader (med eller utan markdown-formattering)
        if ':' in line and len(line.split(':', 1)) == 2:
            speaker_part, text_part = line.split(':', 1)
            
            # Rensa speaker-namnet fr√•n formattering
            speaker_name = clean_text_for_tts(speaker_part).strip()
            
            # Hoppa √∂ver om det inte ser ut som ett talare-namn
            if not speaker_name or len(speaker_name.split()) > 2:
                if current_speaker:
                    current_text.append(clean_text_for_tts(line))
                continue
            
            # Spara f√∂reg√•ende segment
            if current_speaker and current_text:
                clean_text = ' '.join(current_text).strip()
                if clean_text:  # Endast om det finns text
                    # Dela upp l√•nga segment f√∂r TTS-kompatibilitet (max 4000 bytes)
                    split_segments = split_long_text_for_tts(clean_text, current_speaker)
                    segments.extend(split_segments)
            
            # Starta nytt segment
            current_speaker = speaker_name
            text_part_clean = clean_text_for_tts(text_part).strip()
            current_text = [text_part_clean] if text_part_clean else []
        else:
            # Forts√§tt med samma talare
            if current_speaker:
                clean_line = clean_text_for_tts(line)
                if clean_line:  # Endast l√§gg till om det finns text
                    current_text.append(clean_line)
    
    # L√§gg till sista segmentet
    if current_speaker and current_text:
        clean_text = ' '.join(current_text).strip()
        if clean_text:  # Endast om det finns text
            # Dela upp l√•nga segment f√∂r TTS-kompatibilitet (max 4000 bytes)
            split_segments = split_long_text_for_tts(clean_text, current_speaker)
            segments.extend(split_segments)
    
    return segments

def generate_audio_with_gemini_dialog(script_content: str, weather_info: str, output_file: str) -> bool:
    """Generera audio med Gemini TTS f√∂r naturlig dialog mellan Lisa och Pelle"""
    if not GEMINI_TTS_AVAILABLE:
        logger.info("[AUDIO] Gemini TTS inte tillg√§nglig, anv√§nder standard-metod")
        _set_last_gemini_error("GEMINI_TTS_AVAILABLE=False (importfel eller saknade dependencies)")
        return False
    
    try:
        logger.info("[AUDIO] Genererar naturlig dialog med Gemini TTS...")
        
        generator = GeminiTTSDialogGenerator()
        
        # Om manus redan √§r en dialog (Lisa:/Pelle:), anv√§nd det i st√§llet f√∂r att
        # bygga om via heuristisk split (som kan skapa rader utan talarprefix).
        if any(tag in script_content for tag in ("Lisa:", "Pelle:", "LISA:", "PELLE:")):
            parsed_segments = parse_podcast_text(script_content)

            dialog_lines = []
            fallback_speaker = "Lisa"
            for seg in parsed_segments:
                speaker_raw = (seg.get('speaker') or '').strip()
                text = (seg.get('text') or '').strip()
                if not text:
                    continue

                speaker_lower = speaker_raw.lower()
                if 'lisa' in speaker_lower:
                    speaker = "Lisa"
                elif 'pelle' in speaker_lower:
                    speaker = "Pelle"
                else:
                    # Skulle bara intr√§ffa om manus inneh√•ller annan talare.
                    speaker = fallback_speaker
                    fallback_speaker = "Pelle" if fallback_speaker == "Lisa" else "Lisa"

                dialog_lines.append(f"{speaker}: {text}")

            dialog_script = "\n".join(dialog_lines).strip()
            logger.info(f"[AUDIO] Dialog-script byggt fr√•n {len(dialog_lines)} parsade repliker")
        else:
            # Skapa dialog-script fr√•n inneh√•ll (fallback)
            dialog_script = generator.create_dialog_script(script_content, weather_info)
            logger.info("[AUDIO] Dialog-script skapat f√∂r Lisa och Pelle")

        if not dialog_script:
            logger.warning("[AUDIO] Tomt dialog-script f√∂r Gemini, faller tillbaka")
            return False
        
        # Generera audio med freeform dialog
        success = generator.synthesize_dialog_freeform(
            dialog_script=dialog_script,
            output_file=output_file
        )
        
        if success:
            logger.info(f"[AUDIO] ‚úÖ Gemini TTS dialog sparad: {output_file}")
            _set_last_gemini_error(None)
            return True
        else:
            logger.warning("[AUDIO] Gemini TTS misslyckades, faller tillbaka till standard")
            _set_last_gemini_error("synthesize_dialog_freeform returned False (see [GEMINI-TTS] logs)")
            return False
            
    except Exception as e:
        _set_last_gemini_error(f"{type(e).__name__}: {e}")
        logger.exception("[AUDIO] Gemini TTS fel")
        logger.info("[AUDIO] Faller tillbaka till standard TTS")
        return False

def generate_audio_google_cloud(segments: List[Dict], output_file: str) -> bool:
    """Generera audio med Google Cloud TTS (fallback-metod)"""
    try:
        # Anv√§nd r√§tt TTS-klass
        from google_cloud_tts import GoogleCloudTTS
        
        # Konvertera segments till r√§tt format
        audio_segments = []
        for i, segment in enumerate(segments):
            speaker = segment['speaker'].lower()
            
            # Mappa talare till r√∂ster
            if 'lisa' in speaker:
                voice = 'lisa'
            elif 'pelle' in speaker:
                voice = 'pelle'
            else:
                # Alternerande r√∂ster
                voice = 'lisa' if i % 2 == 0 else 'pelle'
            
            audio_segments.append({
                'voice': voice,
                'text': segment['text']
            })
        
        # Skapa TTS-instans och generera audio
        tts = GoogleCloudTTS()
        audio_file = tts.generate_podcast_audio(audio_segments)
        
        if audio_file and os.path.exists(audio_file):
            shutil.move(audio_file, output_file)
            logger.info(f"[TTS] Audio genererad: {output_file}")
            return True
        else:
            logger.error("[ERROR] TTS-generering misslyckades")
            return False
            
    except Exception as e:
        logger.error(f"[ERROR] TTS-fel: {e}")
        return False

def add_music_to_podcast(audio_file: str) -> str:
    """L√§gg till musik och bryggkor till podcast med MusicMixer"""
    try:
        logger.info("[MUSIC] L√§gger till musik och bryggkor...")
        
        # Skapa en MusicMixer-instans
        mixer = MusicMixer()
        
        # Skanna alla mp3-filer i music-mappen automatiskt
        music_dir = "audio/music"
        available_music = []
        
        if os.path.exists(music_dir):
            for filename in os.listdir(music_dir):
                if filename.lower().endswith('.mp3'):
                    full_path = os.path.join(music_dir, filename)
                    available_music.append(full_path)
        
        logger.info(f"[MUSIC] Hittade {len(available_music)} musikfiler: {[os.path.basename(f) for f in available_music]}")
        
        if not available_music:
            logger.warning("[MUSIC] Inga musikfiler hittades")
            return audio_file
            
        # Skapa varierad musikmix f√∂r hela avsnittet
        import random
        music_output = audio_file.replace('.mp3', '_with_music.mp3')
        
        # Nya f√∂rb√§ttrade musikmixning med variation
        success = mixer.create_varied_music_background(
            speech_file=audio_file,
            available_music=available_music,
            output_file=music_output,
            music_volume=-15,  # L√•g volym i dB
            segment_duration=60,  # Byt musik var 60:e sekund
            fade_duration=3000    # 3 sekunder crossfade mellan l√•tar
        )
        
        if success and os.path.exists(music_output):
            # Ers√§tt original med musikversion
            shutil.move(music_output, audio_file)
            logger.info(f"[MUSIC] Musik tillagd till {audio_file}")
            return audio_file
        else:
            logger.warning("[MUSIC] Kunde inte l√§gga till musik, anv√§nder original")
            return audio_file
            
    except Exception as e:
        logger.error(f"[MUSIC] Fel vid musiktill√§gg: {e}")
        return audio_file  # Returnera original om musik misslyckas


def enforce_intro_date(script_text: str, weekday: str, day: int, month: str) -> str:
    """Replace incorrect intro date phrases with the actual Swedish date."""
    if not script_text:
        return script_text

    def _replace_intro(match: re.Match) -> str:
        intro_line = match.group(0)
        comma_index = intro_line.find(',')
        suffix = intro_line[comma_index:] if comma_index != -1 else ''
        logger.info("[SCRIPT] Uppdaterar intro-datum till dagens datum")
        return f"Idag √§r det {weekday} den {day} {month}{suffix}"

    return re.sub(r"Idag √§r det[^\n]*", _replace_intro, script_text, count=1)


def clean_xml_text(text: Optional[str]) -> str:
    """Sanitize text for safe XML output"""
    if not text:
        return ""

    cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', str(text))
    normalized = html.unescape(cleaned)
    return html.escape(normalized, quote=True)


def generate_github_rss(episodes_data: List[Dict], base_url: str) -> str:
    """Generera RSS-feed f√∂r GitHub Pages"""
    rss_items = []
    
    for episode in episodes_data:
        # Hantera b√•da gamla (date) och nya (pub_date) format
        if 'pub_date' in episode:
            pub_date = episode['pub_date']  # Redan i r√§tt format
        elif 'date' in episode:
            pub_date = datetime.strptime(episode['date'], '%Y-%m-%d').strftime('%a, %d %b %Y %H:%M:%S +0000')
        else:
            pub_date = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S +0000')
        
        # Hantera b√•de gamla och nya format f√∂r audio URL och storlek
        if 'audio_url' in episode:
            # Nytt format fr√•n episodhistorik
            audio_url = episode['audio_url']
            file_size = episode.get('file_size', 7000000)
            guid = episode.get('guid', audio_url)
        else:
            # Gammalt format fr√•n direct generation
            audio_url = f"{base_url}/audio/{episode['filename']}"
            file_size = episode.get('size', 7000000)
            guid = audio_url
        
        safe_title = clean_xml_text(episode.get('title', ''))
        safe_description = clean_xml_text(episode.get('description', ''))
        safe_audio_url = html.escape(audio_url, quote=True)
        safe_guid = clean_xml_text(guid)

        rss_items.append(f"""        <item>
            <title>{safe_title}</title>
            <description>{safe_description}</description>
            <pubDate>{pub_date}</pubDate>
            <enclosure url="{safe_audio_url}" length="{file_size}" type="audio/mpeg"/>
            <guid>{safe_guid}</guid>
        </item>""")
    
    rss_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">
    <channel>
        <title>MMM Senaste Nytt - M√ÑNNISKA MASKIN MILJ√ñ</title>
        <description>Dagliga nyheter fr√•n v√§rlden av m√§nniska, maskin och milj√∂ - med Lisa och Pelle. En del av M√§nniska Maskin Milj√∂-familjen.</description>
        <link>{base_url}</link>
        <language>sv-SE</language>
        <itunes:category text="Technology"/>
        <itunes:category text="Science"/>
        <itunes:category text="News"/>
        <itunes:explicit>false</itunes:explicit>
        <itunes:author>Pontus Dahlberg</itunes:author>
        <itunes:summary>Daglig nyhetspodcast om AI, teknik och klimat - en del av M√§nniska Maskin Milj√∂-familjen</itunes:summary>
        <itunes:owner>
            <itunes:name>Pontus Dahlberg</itunes:name>
            <itunes:email>pontus.dahlberg@gmail.com</itunes:email>
        </itunes:owner>
        <itunes:image href="{base_url}/cover.jpg"/>
        <lastBuildDate>{datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')}</lastBuildDate>
        {''.join(rss_items)}
    </channel>
</rss>"""
    
    return rss_content

def main():
    """Huvudfunktion f√∂r komplett podcast-generering med musik och v√§der"""
    logger.info("[PODCAST] Startar MMM Senaste Nytt med musik och v√§der...")
    
    # Log Gemini TTS status after logger is initialized
    if GEMINI_TTS_AVAILABLE:
        logger.info("[SYSTEM] Gemini TTS tillg√§nglig f√∂r naturlig dialog")
    else:
        logger.warning("[SYSTEM] Gemini TTS inte tillg√§nglig - anv√§nder standard TTS")

    log_diagnostic('system_tts_capabilities', {
        'gemini_tts_available': bool(GEMINI_TTS_AVAILABLE),
        'gemini_tts_import_error': GEMINI_TTS_IMPORT_ERROR,
    })
    
    # Log Fact Checker status
    if FACT_CHECKER_AVAILABLE:
        logger.info("[SYSTEM] üõ°Ô∏è KRITISK faktakontroll-agent aktiverad f√∂r s√§kerhet")
    else:
        logger.error("[SYSTEM] ‚ö†Ô∏è VARNING: Faktakontroll-agent inte tillg√§nglig - RISK F√ñR FELAKTIG INFO!")
    
    try:
        # H√§mta v√§derdata f√∂rst
        logger.info("[WEATHER] H√§mtar aktuell v√§derdata...")
        weather_info = get_swedish_weather()
        logger.info(f"[WEATHER] {weather_info}")
        
        # Ladda konfiguration
        config = load_config()
        
        # Generera datum och filnamn
        today = datetime.now()
        timestamp = today.strftime('%Y%m%d_%H%M%S')

        # S√§tt run-id s√• att diagnostics kan korreleras mellan moduler
        set_run_id(timestamp)
        os.environ['MMM_RUN_ID'] = timestamp
        
        # Skapa output-mappar
        os.makedirs('audio', exist_ok=True)
        os.makedirs('public/audio', exist_ok=True)
        
        # Generera strukturerat podcast-inneh√•ll med riktig v√§derdata
        logger.info("[AI] Genererar strukturerat podcast-inneh√•ll...")
        podcast_content, referenced_articles = generate_structured_podcast_content(weather_info, today=today)

        weekday_swedish = SWEDISH_WEEKDAYS.get(today.strftime('%A'), today.strftime('%A'))
        month_swedish = SWEDISH_MONTHS.get(today.month, today.strftime('%B').lower())
        podcast_content = enforce_intro_date(podcast_content, weekday_swedish, today.day, month_swedish)
        
        # Spara manus f√∂r referens
        script_path = f"podcast_script_{timestamp}.txt"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(podcast_content)
        logger.info(f"[SCRIPT] Manus sparat: {script_path}")
        
        # Spara artikelreferenser f√∂r senare anv√§ndning
        articles_path = f"episode_articles_{timestamp}.json"
        with open(articles_path, 'w', encoding='utf-8') as f:
            json.dump(referenced_articles, f, indent=2, ensure_ascii=False)
        logger.info(f"[ARTICLES] Artikelreferenser sparade: {articles_path}")
        
        # üõ°Ô∏è SJ√ÑLVKORRIGERANDE FAKTAKONTROLL - Automatisk korrigering av problem
        final_podcast_content = podcast_content
        max_correction_attempts = 3

        # Sammanfattning som kan anv√§ndas i kvalitetsrapport
        fact_check_summary = {
            'status': 'UNKNOWN',
            'warnings': [],
            'critical_issues_count': None,
            'correction_attempts': 0,
            'auto_correct_used': False,
        }
        
        for correction_attempt in range(max_correction_attempts):
            logger.info(f"[FACT-CHECK] üõ°Ô∏è Faktakontroll f√∂rs√∂k {correction_attempt + 1}/{max_correction_attempts}")
            fact_check_summary['correction_attempts'] = correction_attempt + 1
            
            # Grundl√§ggande faktakontroll f√∂rst (snabbast)
            fact_check_passed = False
            if BASIC_FACT_CHECKER_AVAILABLE:
                basic_checker = BasicFactChecker()
                basic_result = basic_checker.basic_fact_check(final_podcast_content)
                
                if basic_result['safe_to_publish']:
                    # Visa varningar men godk√§nn √§nd√•
                    warnings = basic_result.get('warnings', [])
                    fact_check_summary['warnings'] = warnings
                    fact_check_summary['critical_issues_count'] = 0
                    if warnings:
                        logger.info(f"[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd med varningar: {warnings}")
                    else:
                        logger.info("[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd helt")
                    fact_check_summary['status'] = 'SAFE'
                    fact_check_passed = True
                    break
                else:
                    critical_issues = basic_result.get('critical_issues', [])
                    warnings = basic_result.get('warnings', [])
                    fact_check_summary['warnings'] = warnings
                    fact_check_summary['critical_issues_count'] = len(critical_issues)
                    fact_check_summary['status'] = 'REQUIRES_REVIEW'
                    logger.error(f"[FACT-CHECK] ‚ùå Kritiska problem hittade: {critical_issues}")
                    if warnings:
                        logger.info(f"[FACT-CHECK] ‚ÑπÔ∏è Varningar (blockerar inte): {warnings}")
                    
                    # F√∂rs√∂k automatisk korrigering
                    if SELF_CORRECTING_AVAILABLE and correction_attempt < max_correction_attempts - 1:
                        logger.info("[FACT-CHECK] üîß Startar automatisk korrigering...")
                        
                        corrected_content, correction_success = auto_correct_podcast_content(
                            final_podcast_content, basic_result.get('critical_issues', [])
                        )
                        
                        if correction_success:
                            logger.info("[FACT-CHECK] ‚úÖ Automatisk korrigering lyckades!")
                            final_podcast_content = corrected_content
                            fact_check_summary['auto_correct_used'] = True
                            
                            # Spara korrigerat manus
                            corrected_script_path = f"podcast_script_{timestamp}_corrected_v{correction_attempt + 1}.txt"
                            with open(corrected_script_path, 'w', encoding='utf-8') as f:
                                f.write(final_podcast_content)
                            logger.info(f"[SCRIPT] Korrigerat manus sparat: {corrected_script_path}")
                            continue
                        else:
                            logger.warning("[FACT-CHECK] ‚ö†Ô∏è Automatisk korrigering misslyckades")
                    else:
                        logger.warning("[FACT-CHECK] ‚ö†Ô∏è Sj√§lvkorrigering inte tillg√§nglig")
            
            # Om vi n√•r hit har korrigering misslyckats eller √§r sista f√∂rs√∂ket
            break
        
        # Final kontroll
        if not fact_check_passed:
            logger.error("üö® PUBLICERING STOPPAD - FAKTAKONTROLL MISSLYCKADES!")
            fact_check_summary['status'] = 'FAILED'
            
            # Spara rapport f√∂r manuell granskning
            final_report_path = f"fact_check_failed_{timestamp}.txt"
            with open(final_report_path, 'w', encoding='utf-8') as f:
                f.write("FAKTAKONTROLL MISSLYCKADES\n")
                f.write(f"Datum: {datetime.now().isoformat()}\n")
                f.write(f"F√∂rs√∂k gjorda: {correction_attempt + 1}\n\n")
                if 'basic_result' in locals():
                    f.write("SENASTE PROBLEM:\n")
                    for issue in basic_result['issues_found']:
                        f.write(f"- {issue}\n")
            
            print(f"\nüö® S√ÑKERHETSVARNING: Automatisk korrigering misslyckades!")
            print(f"Rapport sparad: {final_report_path}")
            print("Manuell granskning kr√§vs. Se MANUAL_FACT_CHECK_GUIDE.md")
            return False
        else:
            logger.info("[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd - s√§kert att publicera")
            # Uppdatera inneh√•llet om det korrigerades
            if final_podcast_content != podcast_content:
                logger.info("[FACT-CHECK] üìù Anv√§nder automatiskt korrigerat inneh√•ll")
                podcast_content = final_podcast_content
        
        # Parsa inneh√•llet i segment
        segments = parse_podcast_text(podcast_content)
        logger.info(f"[PARSE] Hittade {len(segments)} segment att generera audio f√∂r")
        
        # R√§kna ord f√∂r att uppskatta l√§ngd
        total_words = sum(len(segment['text'].split()) for segment in segments)
        estimated_minutes = total_words / 150  # Ungef√§r 150 ord per minut i tal
        logger.info(f"[ESTIMATE] {total_words} ord, uppskattad l√§ngd: {estimated_minutes:.1f} minuter")
        
        # Generera filnamn
        audio_filename = f"MMM_senaste_nytt_{timestamp}.mp3"
        audio_filepath = os.path.join('audio', audio_filename)

        require_gemini_tts = os.getenv('MMM_FORCE_GEMINI_TTS', '').strip().lower() in {'1', 'true', 'yes', 'y'}
        
        # F√∂rs√∂k f√∂rst med Gemini TTS f√∂r naturlig dialog
        logger.info("[TTS] F√∂rs√∂ker generera naturlig dialog med Gemini TTS...")
        log_diagnostic('tts_provider_attempt', {
            'provider': 'gemini',
            'output_file': audio_filepath,
            'require_gemini': require_gemini_tts,
        })
        gemini_success = generate_audio_with_gemini_dialog(podcast_content, weather_info, audio_filepath)
        
        if not gemini_success:
            log_diagnostic('tts_provider_result', {
                'provider': 'gemini',
                'success': False,
                'error': _LAST_GEMINI_ERROR,
            })

            if require_gemini_tts:
                logger.error("[TTS] MMM_FORCE_GEMINI_TTS √§r aktivt: avbryter ist√§llet f√∂r fallback")
                return False

            # Fallback till standard Google Cloud TTS
            logger.info("[TTS] Anv√§nder standard Google Cloud TTS som fallback...")
            log_diagnostic('tts_provider_fallback', {
                'from_provider': 'gemini',
                'to_provider': 'google_cloud',
            })
            success = generate_audio_google_cloud(segments, audio_filepath)

            log_diagnostic('tts_provider_result', {
                'provider': 'google_cloud',
                'success': bool(success),
            })
            
            if not success:
                logger.error("[ERROR] Audio-generering misslyckades")
                return False
        else:
            logger.info("[TTS] ‚úÖ Naturlig dialog genererad med Gemini TTS!")
            log_diagnostic('tts_provider_result', {
                'provider': 'gemini',
                'success': True,
            })
        
        # L√§gg till musik och bryggkor
        audio_filepath = add_music_to_podcast(audio_filepath)
        
        # Kopiera till public/audio f√∂r GitHub Pages
        public_audio_path = os.path.join('public', 'audio', audio_filename)
        shutil.copy2(audio_filepath, public_audio_path)
        logger.info(f"[FILES] Kopierade audio till {public_audio_path}")
        
        # Skapa episode data med artiklar som faktiskt refererades i avsnittet
        file_size = os.path.getsize(audio_filepath)
        
        # F√∂r RSS: lista bara k√§llor som sannolikt faktiskt n√§mns i manuset
        logger.info(f"[RSS] Antal kandidat-artiklar: {len(referenced_articles)}")

        rss_referenced_articles = extract_referenced_articles(podcast_content, referenced_articles, max_results=6)
        logger.info(f"[RSS] Antal matchade artiklar i manus: {len(rss_referenced_articles)}")

        # Diagnostics: visa vilka som inte matchades (hj√§lper fels√∂ka "k√§llor som inte var med")
        try:
            matched_keys = set()
            for a in rss_referenced_articles:
                key = _canonicalize_url(a.get('link', '')) or _title_fingerprint(a.get('title', ''))
                if key:
                    matched_keys.add(key)

            unmatched = []
            for a in referenced_articles:
                key = _canonicalize_url(a.get('link', '')) or _title_fingerprint(a.get('title', ''))
                if key and key not in matched_keys:
                    unmatched.append({'source': a.get('source', ''), 'title': a.get('title', ''), 'link': a.get('link', '')})

            if unmatched:
                log_diagnostic('rss_sources_unmatched', {
                    'candidate_count': len(referenced_articles),
                    'matched_count': len(rss_referenced_articles),
                    'unmatched_count': len(unmatched),
                    'examples': unmatched[:10],
                })
        except Exception:
            pass

        article_links = []
        for i, article in enumerate(rss_referenced_articles):
            logger.info(f"[RSS] Artikel {i+1}: {article.get('source', 'N/A')} - {article.get('title', 'N/A')[:50]}...")
            if article.get('link') and article.get('title'):
                short_title = article['title'][:60] + "..." if len(article['title']) > 60 else article['title']
                source_name = article.get('source', 'Ok√§nd k√§lla')
                article_links.append(f"{source_name}: {short_title}\n  {article['link']}")
        
        sources_text = ""
        if article_links:
            sources_text = f"\n\nK√§llor som refereras i detta avsnitt:\n‚Ä¢ " + "\n‚Ä¢ ".join(article_links)
            logger.info(f"[RSS] Lade till {len(article_links)} k√§llor i RSS-beskrivning")
        else:
            logger.warning("[RSS] Inga k√§llor att visa i RSS-beskrivning!")
        
        month_swedish = SWEDISH_MONTHS[today.month]
        weekday_swedish = SWEDISH_WEEKDAYS.get(today.strftime('%A'), today.strftime('%A'))
        
        episode_data = {
            'title': f"MMM Senaste Nytt - {today.day} {month_swedish} {today.year}",
            'description': f"Dagens nyheter inom AI, teknik och klimat - {weekday_swedish} den {today.day} {month_swedish} {today.year}. Med detaljerade k√§llh√§nvisningar fr√•n svenska och internationella medier.{sources_text}",
            'date': today.strftime('%Y-%m-%d'),
            'filename': audio_filename,
            'size': file_size,
            'duration': f"{estimated_minutes:.0f}:00"
        }
        
        # L√§gg till episod till historik och generera RSS-feed med alla episoder
        logger.info("[HISTORY] L√§gger till episod till historik...")
        history = EpisodeHistory()
        all_episodes = history.add_episode(episode_data)
        
        # Generera RSS-feed med alla episoder (max 10 senaste f√∂r RSS-prestanda)
        base_url = "https://pontusdahlberg.github.io/morgonpodden"
        recent_episodes = all_episodes[:10]  # Ta bara 10 senaste f√∂r RSS-feed
        rss_content = generate_github_rss(recent_episodes, base_url)
        
        # Spara RSS-feed
        rss_path = os.path.join('public', 'feed.xml')
        with open(rss_path, 'w', encoding='utf-8') as f:
            f.write(rss_content)
        logger.info(f"[RSS] RSS-feed sparad med {len(recent_episodes)} episoder: {rss_path}")

        # ============================================================
        # KVALITETSRAPPORT (relevans/korrekthet/k√∂rningsproblem)
        # ============================================================
        try:
            from src.episode_quality import generate_episode_quality_report, write_quality_reports

            scraped_data_for_report = None
            try:
                with open('scraped_content.json', 'r', encoding='utf-8') as f:
                    scraped_data_for_report = json.load(f)
            except Exception:
                scraped_data_for_report = None

            quality_report = generate_episode_quality_report(
                run_id=timestamp,
                script_text=podcast_content,
                referenced_articles=referenced_articles,
                scraped_content=scraped_data_for_report,
                diagnostics_file=DIAGNOSTICS_FILE,
                fact_check_summary=fact_check_summary,
            )
            paths = write_quality_reports(report=quality_report, output_dir='episodes')
            logger.info(f"[QUALITY] Rapport sparad: {paths.get('markdown')} | {paths.get('json')}")
        except Exception as e:
            logger.warning(f"[QUALITY] Kunde inte skapa kvalitetsrapport: {e}")
        
        # Logga framg√•ng
        logger.info("[SUCCESS] Komplett podcast-generering slutf√∂rd!")
        logger.info(f"[AUDIO] Audio: {audio_filepath}")
        logger.info(f"[SCRIPT] Manus: {script_path}")
        logger.info(f"[RSS] RSS: {rss_path}")
        logger.info(f"[WEATHER] V√§der: {weather_info}")
        logger.info(f"[STATS] Segment: {len(segments)}, Ord: {total_words}, L√§ngd: ~{estimated_minutes:.1f} min")
        logger.info(f"[GITHUB] GitHub Pages URL: {base_url}")
        logger.info(f"[FEED] RSS URL: {base_url}/feed.xml")
        
        return True
        
    except Exception as e:
        logger.error(f"[ERROR] Ov√§ntat fel: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == '__main__':
        if any(arg in {'-h', '--help'} for arg in sys.argv[1:]):
                print(
                        """Usage:
    python run_podcast_complete.py

Genererar MMM Senaste Nytt (nyheter + v√§der + TTS + musik + RSS).

Valfria env vars (fels√∂kning):
    MMM_FORCE_GEMINI_TTS=1
        - Avbryter k√∂rningen om Gemini-TTS misslyckas (ingen tyst fallback).

    GEMINI_TTS_PROMPT_MAX_BYTES=850
        - Maxstorlek f√∂r prompt (UTF-8 bytes).

    GEMINI_TTS_MAX_BYTES=3900
        - Maxstorlek per chunk i TTS-input (UTF-8 bytes).
"""
                )
                sys.exit(0)

        success = main()
        sys.exit(0 if success else 1)