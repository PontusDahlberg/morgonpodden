#!/usr/bin/env python3
"""
Komplett podcast-generator f√∂r MMM Senaste Nytt
Med musik-integration och riktig v√§derdata
"""

import os
import sys
import json
import logging
import shutil
import requests
import re
import html
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple
from dotenv import load_dotenv

# L√§gg till modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from music_mixer import MusicMixer
from episode_history import EpisodeHistory

# Import Gemini TTS f√∂r f√∂rb√§ttrad dialog
try:
    from gemini_tts_dialog import GeminiTTSDialogGenerator
    GEMINI_TTS_AVAILABLE = True
    GEMINI_TTS_IMPORT_ERROR: Optional[str] = None
except ImportError as e:
    GEMINI_TTS_AVAILABLE = False
    GEMINI_TTS_IMPORT_ERROR = str(e)

# Import KRITISK faktakontroll-agent
try:
    from news_fact_checker import NewsFactChecker
    FACT_CHECKER_AVAILABLE = True
except ImportError as e:
    FACT_CHECKER_AVAILABLE = False

# Import backup faktakontroll (fungerar utan AI)
try:
    from basic_fact_checker import BasicFactChecker, quick_fact_check
    BASIC_FACT_CHECKER_AVAILABLE = True
except ImportError as e:
    BASIC_FACT_CHECKER_AVAILABLE = False

# Import sj√§lvkorrigerande faktakontroll
try:
    from self_correcting_fact_checker import SelfCorrectingFactChecker, auto_correct_podcast_content
    SELF_CORRECTING_AVAILABLE = True
except ImportError as e:
    SELF_CORRECTING_AVAILABLE = False

# Ladda milj√∂variabler
load_dotenv()
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# Konfigurera logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('podcast_generation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

SWEDISH_WEEKDAYS = {
    'Monday': 'm√•ndag',
    'Tuesday': 'tisdag',
    'Wednesday': 'onsdag',
    'Thursday': 'torsdag',
    'Friday': 'fredag',
    'Saturday': 'l√∂rdag',
    'Sunday': 's√∂ndag'
}

SWEDISH_MONTHS = {
    1: 'januari',
    2: 'februari',
    3: 'mars',
    4: 'april',
    5: 'maj',
    6: 'juni',
    7: 'juli',
    8: 'augusti',
    9: 'september',
    10: 'oktober',
    11: 'november',
    12: 'december'
}

# Stopwords f√∂r enkel titel-fingerprinting (f√∂r att undvika dagliga upprepningar)
_TITLE_STOPWORDS = {
    'och', 'eller', 'men', 'att', 'som', 'det', 'den', 'detta', 'dessa', 'en', 'ett', 'i', 'p√•', 'av', 'till',
    'f√∂r', 'med', 'utan', '√∂ver', 'under', 'efter', 'f√∂re', 'om', 'n√§r', 'd√§r', 'h√§r', 'fr√•n', 'mot',
    's√§ger', 'sa', 'uppger', 'enligt', 'nya', 'ny', 'nu', 'idag', 'ig√•r', 'imorgon',
    'the', 'a', 'an', 'and', 'or', 'but', 'to', 'of', 'in', 'on', 'for', 'with', 'from', 'by', 'as', 'at',
}

# Diagnostics
DIAGNOSTICS_FILE = os.getenv('MMM_DIAGNOSTICS_FILE', 'diagnostics.jsonl')
_CURRENT_RUN_ID: Optional[str] = None
_LAST_GEMINI_ERROR: Optional[str] = None


def set_run_id(run_id: str) -> None:
    global _CURRENT_RUN_ID
    _CURRENT_RUN_ID = run_id


def log_diagnostic(event: str, payload: Dict) -> None:
    """Skriv en strukturerad rad till diagnostics.jsonl utan att spamma huvudloggen."""
    try:
        entry = {
            'ts': datetime.now().isoformat(timespec='seconds'),
            'run_id': _CURRENT_RUN_ID,
            'event': event,
            **payload,
        }
        with open(DIAGNOSTICS_FILE, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception:
        # Diagnostik f√•r aldrig stoppa k√∂rningen
        pass


def _write_run_diagnostics_extract(
    *,
    run_id: str,
    diagnostics_file: str = DIAGNOSTICS_FILE,
    output_dir: str = 'episodes',
    max_lines: int = 2000,
) -> Optional[str]:
    """Write a filtered diagnostics.jsonl containing only entries for this run_id."""
    try:
        if not run_id:
            return None
        if not os.path.exists(diagnostics_file):
            return None
        os.makedirs(output_dir, exist_ok=True)
        out_path = os.path.join(output_dir, f"diagnostics_{run_id}.jsonl")

        kept = 0
        with open(diagnostics_file, 'r', encoding='utf-8') as fin, open(out_path, 'w', encoding='utf-8') as fout:
            for line in fin:
                if not line.strip():
                    continue
                # Fast path: don't parse JSON unless likely match
                if f'"run_id": "{run_id}"' not in line and f'"run_id":"{run_id}"' not in line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue
                if obj.get('run_id') != run_id:
                    continue
                fout.write(json.dumps(obj, ensure_ascii=False) + "\n")
                kept += 1
                if kept >= max(1, int(max_lines)):
                    break

        if kept <= 0:
            try:
                os.remove(out_path)
            except Exception:
                pass
            return None
        return out_path
    except Exception:
        return None


def _write_log_tail(
    *,
    run_id: str,
    log_path: str = 'podcast_generation.log',
    output_dir: str = 'episodes',
    max_lines: int = 600,
) -> Optional[str]:
    """Write a short tail of the main log file for email attachment."""
    try:
        if not run_id:
            return None
        if not os.path.exists(log_path):
            return None
        os.makedirs(output_dir, exist_ok=True)
        out_path = os.path.join(output_dir, f"log_tail_{run_id}.txt")

        with open(log_path, 'r', encoding='utf-8', errors='replace') as f:
            lines = f.readlines()
        tail = lines[-max(50, int(max_lines)):] if lines else []
        with open(out_path, 'w', encoding='utf-8') as out:
            out.write("".join(tail))
        return out_path
    except Exception:
        return None


def _set_last_gemini_error(message: Optional[str]) -> None:
    global _LAST_GEMINI_ERROR
    _LAST_GEMINI_ERROR = message


def _canonicalize_url(url: str) -> str:
    """Skapa stabil URL-nyckel som inte p√•verkas av tracking-parametrar."""
    if not url:
        return ""
    try:
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

        parsed = urlparse(url.strip())
        query_pairs = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True)
                       if not k.lower().startswith('utm_') and k.lower() not in {'fbclid', 'gclid'}]
        cleaned = parsed._replace(query=urlencode(query_pairs, doseq=True), fragment="")

        # Normalisera bort trailing slash
        normalized = urlunparse(cleaned).rstrip('/')
        return normalized
    except Exception:
        return url.strip().rstrip('/')


def _title_fingerprint(title: str) -> str:
    """Skapa en grov "fingerprint" av en titel f√∂r dedupe mellan dagar."""
    if not title:
        return ""
    text = title.lower()
    text = re.sub(r'[^\w\s√•√§√∂√Ö√Ñ√ñ-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = [t for t in text.split(' ') if len(t) >= 4 and t not in _TITLE_STOPWORDS and not t.isdigit()]
    if not tokens:
        return ""
    # Sortera och ta en begr√§nsad m√§ngd token f√∂r stabilitet
    tokens = sorted(set(tokens))
    return ' '.join(tokens[:12])


def _is_follow_up_article(title: str, content: str) -> bool:
    """Heuristik: uppf√∂ljningar √§r okej, men daglig repetition ska bort."""
    text = f"{title} {content}".lower()
    # Viktigt: h√•ll detta strikt. M√•nga rubriker inneh√•ller ord som "nu"/"senaste" utan att vara
    # en verklig uppf√∂ljning, vilket annars g√∂r att samma nyheter slinker igenom dag efter dag.
    follow_up_markers = [
        'uppf√∂lj', 'fortsatt', 'ytterligare', 'nya uppgifter', 'uppdater',
        'r√§tteg√•ng', 'dom', 'd√∂md', '√∂verklag', '√•tal', '√•talad', 'f√§ll', 'frias', 'utred',
        'follow-up', 'update', 'new details'
    ]
    return any(m in text for m in follow_up_markers)


def _format_swedish_date(dt: datetime) -> str:
    month_swedish = SWEDISH_MONTHS.get(dt.month, dt.strftime('%B').lower())
    return f"{dt.day} {month_swedish} {dt.year}"


def _load_recent_episode_articles(within_days: int, today: datetime) -> List[Dict[str, Any]]:
    """Load recent episode_articles_*.json entries.

    Returns a flat list of dicts:
    { 'date': datetime, 'source': str, 'title': str, 'link': str, 'content': str }
    """
    try:
        import glob

        cutoff_date = (today - timedelta(days=within_days)).date()
        results: List[Dict[str, Any]] = []
        for article_file in glob.glob('episode_articles_*.json'):
            try:
                # episode_articles_20251014_190405.json
                date_part = article_file.split('_')[2]
                file_date = datetime.strptime(date_part, '%Y%m%d').date()
                if file_date < cutoff_date:
                    continue
            except Exception:
                continue

            try:
                with open(article_file, 'r', encoding='utf-8') as f:
                    prev_articles = json.load(f) or []
                for a in prev_articles:
                    results.append({
                        'date': datetime.combine(file_date, datetime.min.time()),
                        'source': (a.get('source') or '').strip(),
                        'title': (a.get('title') or '').strip(),
                        'link': (a.get('link') or '').strip(),
                        'content': (a.get('content') or '').strip(),
                    })
            except Exception as e:
                logger.warning(f"[HISTORY] Kunde inte l√§sa {article_file}: {e}")
        return results
    except Exception:
        return []


def _build_previous_coverage_hints(
    current_articles: List[Dict[str, Any]],
    recent_episode_articles: List[Dict[str, Any]],
    today: datetime,
    max_hints: int = 3,
    min_overlap_tokens: int = 4,
    min_jaccard: float = 0.50,
    informative_max_freq: int = 2,
    min_anchor_token_len: int = 7,
    debug: bool = False,
    debug_max_samples: int = 6,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """Find conservative "topic" matches between today's curated articles and recent episode archives.

    Goal: enable safe callbacks like "senast vi var inne p√• √§mnet".
    We only emit hints when we have high confidence (rare anchor token overlap + similarity).
    """
    debug_info: Dict[str, Any] = {
        'current_considered': 0,
        'recent_items': 0,
        'evaluated_pairs': 0,
        'reasons': {
            'no_title_tokens': 0,
            'no_anchor_tokens': 0,
            'identical_url': 0,
            'identical_title_fp': 0,
            'prev_already_used': 0,
            'prev_is_today_or_future': 0,
            'overlap_below_min': 0,
            'no_anchor_overlap': 0,
            'similarity_below_min': 0,
            'no_candidate': 0,
            'hint_emitted': 0,
        },
        'samples': [],
    }

    if not current_articles or not recent_episode_articles:
        return [], debug_info

    def _token_set_from_title(title: str) -> set:
        fp = _title_fingerprint(title)
        return set(fp.split(' ')) if fp else set()

    def _jaccard(a: set, b: set) -> float:
        if not a or not b:
            return 0.0
        inter = len(a & b)
        union = len(a | b)
        return (inter / union) if union else 0.0

    # Precompute token sets for recent items + token frequency for "rarity".
    recent: List[Tuple[Dict[str, Any], set]] = []
    token_freq: Dict[str, int] = {}
    for r in recent_episode_articles:
        tset = _token_set_from_title(r.get('title', ''))
        if not tset:
            continue
        recent.append((r, tset))
        for t in tset:
            token_freq[t] = token_freq.get(t, 0) + 1

    debug_info['recent_items'] = len(recent)

    hints: List[Dict[str, Any]] = []
    used_prev_keys: set = set()

    for cur in (current_articles or [])[:10]:
        debug_info['current_considered'] += 1
        cur_title = (cur.get('title') or '').strip()
        cur_link = (cur.get('link') or '').strip()
        cur_source = (cur.get('source') or '').strip()

        cur_tokens = _token_set_from_title(cur_title)
        if not cur_tokens:
            debug_info['reasons']['no_title_tokens'] += 1
            continue

        cur_url_key = _canonicalize_url(cur_link)
        cur_fp = _title_fingerprint(cur_title)

        # Identify "anchor" tokens that are rare in the recent archive.
        cur_informative = {
            t for t in cur_tokens
            if token_freq.get(t, 0) <= informative_max_freq and len(t) >= min_anchor_token_len
        }
        if not cur_informative:
            debug_info['reasons']['no_anchor_tokens'] += 1
            continue

        best = None
        best_score = 0.0
        best_date = None
        best_overlap_info = 0

        # For diagnostics: what's the closest we got, even if not passing thresholds.
        best_any_score = 0.0
        best_any_prev_source = ""
        best_any_prev_date = None

        for prev, prev_tokens in recent:
            prev_title = (prev.get('title') or '').strip()
            prev_link = (prev.get('link') or '').strip()

            debug_info['evaluated_pairs'] += 1

            # Avoid identical items (dedupe already tries, but keep this strict)
            if cur_url_key and cur_url_key == _canonicalize_url(prev_link):
                debug_info['reasons']['identical_url'] += 1
                continue
            if cur_fp and cur_fp == _title_fingerprint(prev_title):
                debug_info['reasons']['identical_title_fp'] += 1
                continue

            # Avoid mentioning the same historical item multiple times
            prev_key = _canonicalize_url(prev_link) or _title_fingerprint(prev_title)
            if prev_key and prev_key in used_prev_keys:
                debug_info['reasons']['prev_already_used'] += 1
                continue

            # Avoid "today" as "previous"
            try:
                prev_date = prev.get('date')
                if isinstance(prev_date, datetime) and prev_date.date() >= today.date():
                    debug_info['reasons']['prev_is_today_or_future'] += 1
                    continue
            except Exception:
                pass

            overlap_total = len(cur_tokens & prev_tokens)
            if overlap_total < min_overlap_tokens:
                debug_info['reasons']['overlap_below_min'] += 1
                continue

            overlap_info = len(cur_informative & prev_tokens)
            if overlap_info < 1:
                debug_info['reasons']['no_anchor_overlap'] += 1
                continue

            score = _jaccard(cur_tokens, prev_tokens)

            if score > best_any_score:
                best_any_score = score
                best_any_prev_source = (prev.get('source') or '').strip()
                prev_dt_any = prev.get('date')
                if isinstance(prev_dt_any, datetime):
                    best_any_prev_date = prev_dt_any.date()
                else:
                    best_any_prev_date = None

            if score < min_jaccard:
                debug_info['reasons']['similarity_below_min'] += 1
                continue

            # Prefer the most recent prior mention ("f√∂rra g√•ngen").
            prev_dt = prev.get('date')
            prev_date_key = None
            if isinstance(prev_dt, datetime):
                prev_date_key = prev_dt.date()

            if best is None:
                best = prev
                best_score = score
                best_date = prev_date_key
                best_overlap_info = overlap_info
            else:
                # Pick newer; break ties by stronger informative overlap, then higher similarity.
                if prev_date_key and (best_date is None or prev_date_key > best_date):
                    best = prev
                    best_score = score
                    best_date = prev_date_key
                    best_overlap_info = overlap_info
                elif prev_date_key == best_date:
                    if overlap_info > best_overlap_info:
                        best = prev
                        best_score = score
                        best_overlap_info = overlap_info
                    elif overlap_info == best_overlap_info and score > best_score:
                        best = prev
                        best_score = score

        if best and best_score >= min_jaccard:
            prev_date = best.get('date')
            if isinstance(prev_date, datetime):
                prev_date_str = _format_swedish_date(prev_date)
            else:
                prev_date_str = str(prev_date) if prev_date else "(ok√§nt datum)"

            prev_title = (best.get('title') or '').strip()
            prev_source = (best.get('source') or '').strip()
            prev_link = (best.get('link') or '').strip()

            hints.append({
                'current_source': cur_source,
                'current_title': cur_title,
                'current_link': cur_link,
                'topic_tokens': ' '.join(sorted(list(cur_informative))[:3]),
                'previous_date': prev_date_str,
                'previous_source': prev_source,
                'previous_title': prev_title,
                'previous_link': prev_link,
                'score': round(best_score, 3),
            })

            prev_key = _canonicalize_url(prev_link) or _title_fingerprint(prev_title)
            if prev_key:
                used_prev_keys.add(prev_key)

            debug_info['reasons']['hint_emitted'] += 1
        else:
            debug_info['reasons']['no_candidate'] += 1
            if debug and len(debug_info.get('samples', [])) < max(0, int(debug_max_samples)):
                debug_info['samples'].append({
                    'current_source': cur_source,
                    'current_title': _truncate_text(cur_title, 120),
                    'anchor_tokens': ' '.join(sorted(list(cur_informative))[:3]),
                    'best_any_score': round(best_any_score, 3),
                    'best_any_prev_source': best_any_prev_source,
                    'best_any_prev_date': str(best_any_prev_date) if best_any_prev_date else None,
                })

        if len(hints) >= max_hints:
            break

    # Sort by recency first (best effort), then score.
    def _hint_sort_key(h: Dict[str, Any]) -> tuple:
        try:
            # previous_date is a Swedish string; sorting by score is fine for secondary
            return (h.get('score', 0.0),)
        except Exception:
            return (0.0,)

    hints.sort(key=_hint_sort_key, reverse=True)
    return hints[:max_hints], debug_info


def extract_referenced_articles(podcast_content: str, candidate_articles: List[Dict], max_results: int = 8) -> List[Dict]:
    """F√∂rs√∂k avg√∂ra vilka av kandidat-artiklarna som faktiskt refereras i manuset.

    Vi matchar p√• stabila titel-token (inte exakta str√§ngar) f√∂r att undvika att RSS listar k√§llor
    som aldrig n√§mndes i avsnittet.
    """
    if not podcast_content or not candidate_articles:
        return []

    haystack = podcast_content.lower()
    haystack = re.sub(r'[^\w\s√•√§√∂√Ö√Ñ√ñ-]', ' ', haystack)
    haystack = re.sub(r'\s+', ' ', haystack)

    scored: List[tuple[int, Dict]] = []
    for article in candidate_articles:
        title = (article.get('title') or '').strip()
        source = (article.get('source') or '').strip()
        fp = _title_fingerprint(title)
        if not fp:
            continue

        tokens = fp.split(' ')
        token_hits = sum(1 for t in tokens if t in haystack)
        source_hit = 1 if source and source.lower() in haystack else 0
        score = token_hits + source_hit

        # Kr√§ver minst en rimlig tr√§ff f√∂r att r√§knas
        if score >= 2 or (score >= 1 and any(len(t) >= 10 and t in haystack for t in tokens)):
            scored.append((score, article))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [a for _, a in scored[:max_results]]

def get_swedish_weather() -> str:
    """H√§mta aktuell v√§derdata fr√•n SMHI f√∂r svenska landskap"""
    try:
        # Importera SMHI-modulen
        from smhi_weather import SMHIWeatherService
        
        service = SMHIWeatherService()
        weather_summary = service.get_swedish_weather_summary()
        
        logger.info(f"[WEATHER] {weather_summary}")
        return weather_summary
            
    except Exception as e:
        logger.error(f"[WEATHER] Fel vid SMHI v√§der-h√§mtning: {e}")
        # Fallback till wttr.in om SMHI misslyckas
        try:
            logger.info("[WEATHER] Anv√§nder wttr.in som backup")
            # Svenska landskap med representativa st√§der - alla 4 regioner
            regions = [
                ("G√∂taland", "Goteborg"),
                ("Svealand", "Stockholm"),  
                ("S√∂dra Norrland", "Sundsvall"),
                ("Norra Norrland", "Kiruna")
            ]
            
            weather_data = []
            for region_name, api_name in regions:
                try:
                    url = f"https://wttr.in/{api_name}?format=%C+%t"
                    response = requests.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        weather_info = response.text.strip()
                        weather_data.append(f"{region_name}: {weather_info}")
                        
                except Exception:
                    continue
            
            if weather_data:
                weather_text = f"V√§dret idag: {', '.join(weather_data)}"  # Visa alla regioner
                return weather_text
            else:
                return "V√§dret idag: Varierande v√§derf√∂rh√•llanden √∂ver Sverige"
                
        except Exception:
            return "V√§dret idag: Varierande v√§derf√∂rh√•llanden √∂ver Sverige"

def load_config() -> Dict:
    """Ladda konfiguration fr√•n sources.json"""
    try:
        with open('sources.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error("[ERROR] sources.json hittades inte")
        return {}
    except json.JSONDecodeError as e:
        logger.error(f"[ERROR] Fel i sources.json: {e}")
        return {}

def get_openrouter_response(messages: List[Dict], model: str = "openai/gpt-4o-mini") -> str:
    """Skicka f√∂rfr√•gan till OpenRouter API.

    Om OPENROUTER_API_KEY saknas men OPENAI_API_KEY finns, fall tillbaka till OpenAI direkt.
    Detta f√∂rhindrar att pipeline hamnar i kort 'fallback'-manus p.g.a. saknad nyckel.
    """
    api_key = os.getenv('OPENROUTER_API_KEY')
    if api_key:
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://github.com/PontusDahlberg",
            "X-Title": "MMM Podcast Generator",
            "Content-Type": "application/json"
        }

        data = {
            "model": model,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 4000
        }

        try:
            response = requests.post(url, headers=headers, json=data, timeout=90)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
        except requests.RequestException as e:
            logger.error(f"[ERROR] OpenRouter API error: {e}")
            raise

    openai_api_key = os.getenv('OPENAI_API_KEY')
    if openai_api_key:
        try:
            from openai import OpenAI

            # OpenRouter-modellnamn kan vara t.ex. "openai/gpt-4o-mini".
            # F√∂r OpenAI vill vi oftast ha "gpt-4o-mini".
            default_openai_model = model.split('/', 1)[1] if '/' in model else model
            openai_model = os.getenv('OPENAI_MODEL', '').strip() or default_openai_model

            client = OpenAI(api_key=openai_api_key)
            resp = client.chat.completions.create(
                model=openai_model,
                messages=messages,
                temperature=0.7,
                max_tokens=4000,
            )
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            logger.error(f"[ERROR] OpenAI API error (fallback): {type(e).__name__}: {e}")
            raise

    raise ValueError("OPENROUTER_API_KEY saknas i milj√∂variabler (och OPENAI_API_KEY saknas ocks√•)")


def _count_words(text: str) -> int:
    if not text:
        return 0
    return len(re.findall(r"\S+", text))


def _parse_weekday_value(value: Any) -> Optional[int]:
    """Parse weekday to Python weekday int (Mon=0..Sun=6)."""
    if value is None:
        return None
    if isinstance(value, int):
        if 0 <= value <= 6:
            return value
        return None
    if isinstance(value, str):
        raw = value.strip().lower()
        if not raw:
            return None
        if raw.isdigit():
            try:
                n = int(raw)
            except ValueError:
                return None
            return n if 0 <= n <= 6 else None

        # English + Swedish (common variants)
        mapping = {
            'monday': 0, 'mon': 0, 'm√•ndag': 0,
            'tuesday': 1, 'tue': 1, 'tisdag': 1,
            'wednesday': 2, 'wed': 2, 'onsdag': 2,
            'thursday': 3, 'thu': 3, 'torsdag': 3,
            'friday': 4, 'fri': 4, 'fredag': 4,
            'saturday': 5, 'sat': 5, 'l√∂rdag': 5, 'lordag': 5,
            'sunday': 6, 'sun': 6, 's√∂ndag': 6, 'sondag': 6,
        }
        return mapping.get(raw)

    return None


def _aftertalk_config_for_today(today: datetime, config: Dict) -> Dict[str, Any]:
    """Return effective aftertalk config if it should run today, else {}."""
    podcast_settings = (config or {}).get('podcastSettings') or {}
    aftertalk = (podcast_settings.get('aftertalk') or {}) if isinstance(podcast_settings, dict) else {}

    enabled = bool(aftertalk.get('enabled', False))
    if not enabled:
        return {}

    weekdays_raw = aftertalk.get('weekdays', [])
    weekdays: List[int] = []
    if isinstance(weekdays_raw, (list, tuple)):
        for w in weekdays_raw:
            parsed = _parse_weekday_value(w)
            if parsed is not None:
                weekdays.append(parsed)

    # Default: Saturdays (5)
    if not weekdays:
        weekdays = [5]

    if today.weekday() not in set(weekdays):
        return {}

    # Clamp seconds to sane bounds
    def _as_int(v: Any, default: int) -> int:
        try:
            return int(v)
        except Exception:
            return default

    target_s = _as_int(aftertalk.get('target_seconds'), 120)
    min_s = _as_int(aftertalk.get('min_seconds'), max(45, target_s - 30))
    max_s = _as_int(aftertalk.get('max_seconds'), target_s + 30)
    min_s = max(30, min_s)
    max_s = max(min_s, max_s)

    style = (aftertalk.get('style') or '').strip()
    return {
        'enabled': True,
        'weekdays': weekdays,
        'target_seconds': target_s,
        'min_seconds': min_s,
        'max_seconds': max_s,
        'style': style,
    }


def _split_outro_block(script_text: str) -> tuple[str, str]:
    """Best-effort split of (body, outro) so padding can be inserted before outro."""
    if not script_text:
        return "", ""
    lines = script_text.splitlines()
    if not lines:
        return script_text, ""

    outro_markers = (
        'tack f√∂r att du lyssnade',
        'tack f√∂r idag',
        'det var dagens',
        'vi √§r ai',
        'ai-r√∂ster',
        'dubbelkolla',
        'avsnittsinformationen',
        'm√§nniska maskin milj√∂',
        'mmm senaste nytt',
    )

    last_match = None
    for i in range(len(lines) - 1, -1, -1):
        s = lines[i].strip().lower()
        if not s:
            continue
        if s.startswith('lisa:') or s.startswith('pelle:'):
            if any(m in s for m in outro_markers):
                last_match = i
                break

    if last_match is None:
        return script_text, ""

    # Expand backwards a bit to capture the rest of the outro block.
    start = last_match
    max_back = 10
    while start > 0 and max_back > 0:
        prev = lines[start - 1].strip().lower()
        if not prev:
            start -= 1
            max_back -= 1
            continue
        if prev.startswith('lisa:') or prev.startswith('pelle:'):
            # Stop if it looks like we are back into dense main content.
            if len(prev) > 260 and not any(m in prev for m in outro_markers):
                break
            start -= 1
            max_back -= 1
            continue
        break

    body = "\n".join(lines[:start]).rstrip()
    outro = "\n".join(lines[start:]).lstrip()
    return body, outro


def _fallback_collect_articles_from_scraped(max_per_source: int = 5, max_total: int = 30) -> List[Dict]:
    """Fallback: plocka ett litet urval artiklar direkt fr√•n scraped_content.json."""
    available_articles: List[Dict] = []
    try:
        with open('scraped_content.json', 'r', encoding='utf-8') as f:
            scraped_data = json.load(f)
            for source_group in scraped_data:
                source_name = source_group.get('source', 'Ok√§nd')
                items = source_group.get('items', [])
                for item in items[:max_per_source]:
                    if item.get('link') and item.get('title'):
                        available_articles.append({
                            'source': source_name,
                            'title': item['title'][:100],
                            'content': item.get('content', '')[:300],
                            'link': item['link']
                        })
                    if len(available_articles) >= max_total:
                        return available_articles
    except Exception as e:
        logger.error(f"‚ùå Fallback-filtrering misslyckades: {e}")
    return available_articles


def _truncate_text(text: str, max_chars: int) -> str:
    if not text:
        return ""
    clean = " ".join(str(text).split())
    if len(clean) <= max_chars:
        return clean
    return clean[: max(0, max_chars - 1)].rstrip() + "‚Ä¶"


def _append_article_padding(script_text: str, articles: List[Dict], min_words: int) -> str:
    """Pad:ar upp manuset med k√§llbaserad dialog tills vi n√•r min_words.

    Viktigt: vi hittar inte p√• fakta; vi √•terger endast utdrag ur artikelns content-f√§lt och
    s√§ger tydligt n√§r detaljer saknas.
    """
    if not script_text:
        script_text = ""

    current_wc = _count_words(script_text)
    if current_wc >= min_words:
        return script_text

    body, outro = _split_outro_block(script_text)

    # Undvik att pad:en blir o√§ndlig om articles √§r tom.
    if not articles:
        filler = (
            "\n\nLisa: Innan vi rundar av ‚Äì en sista p√•minnelse: kolla l√§nkarna i avsnittsbeskrivningen f√∂r att verifiera uppgifterna.\n"
            "Pelle: Och h√∂r g√§rna av dig om du uppt√§cker n√•got som beh√∂ver r√§ttas.\n"
        )
        combined = (body + filler).strip() if body else (script_text + filler).strip()
        return (combined + ("\n\n" + outro if outro else "")).strip()

    # Bygg en ‚Äúsnabba nyheter‚Äù-sektion som n√§mner k√§lla + titel-token f√∂r b√§ttre RSS-match.
    lines: List[str] = []
    lines.append("\n\nLisa: Innan vi sammanfattar och rundar av ‚Äì h√§r √§r n√•gra snabba nyheter f√∂r att f√• med helheten i fl√∂det.")
    lines.append("Pelle: Perfekt ‚Äì k√∂r! Och s√§g g√§rna tydligt vilken k√§lla det kommer fr√•n.")

    # Iterera artiklar flera varv om det beh√∂vs, men med h√•rd cap.
    max_items = min(len(articles), 12)
    safe_articles = articles[:max_items]
    loops = 0
    base_text = (body if body else script_text)
    while _count_words(base_text + "\n" + "\n".join(lines)) < min_words and loops < 3:
        for a in safe_articles:
            if _count_words(base_text + "\n" + "\n".join(lines)) >= min_words:
                break
            source = (a.get('source') or 'Ok√§nd k√§lla').strip()
            title = (a.get('title') or '').strip()
            content_snip = _truncate_text(a.get('content', ''), 700)
            if not content_snip:
                content_snip = "Vi har begr√§nsade detaljer i underlaget just nu, men rubriken pekar p√• en tydlig utveckling."

            lines.append(f"\nLisa: N√§sta punkt ‚Äì {source} har en artikel med rubriken \"{_truncate_text(title, 160)}\".")
            lines.append("Pelle: Okej, vad √§r det viktigaste vi ska ta med oss?")
            lines.append(
                "Lisa: "
                + content_snip
                + ("" if content_snip.endswith(('.', '!', '?')) else ".")
            )
            lines.append(
                "Pelle: Bra. Och om det saknas siffror eller detaljer i k√§llan s√• √§r det helt okej ‚Äì d√• s√§ger vi bara det.")
        loops += 1

    combined = (base_text + "\n" + "\n".join(lines)).strip()
    return (combined + ("\n\n" + outro if outro else "")).strip()


def _should_pad_short_scripts() -> bool:
    return os.getenv('MMM_PAD_SHORT_SCRIPTS', '0').strip().lower() in {'1', 'true', 'yes'}

def generate_structured_podcast_content(weather_info: str, today: Optional[datetime] = None) -> tuple[str, List[Dict]]:
    """Generera strukturerat podcast-inneh√•ll med AI och riktig v√§derdata"""
    
    # Dagens datum f√∂r kontext
    today = today or datetime.now()

    # Eftertalk/eftersnack styrs av sources.json (s√• att GitHub Actions kan styra beteendet)
    config = load_config()
    aftertalk_cfg = _aftertalk_config_for_today(today, config)
    date_str = today.strftime('%Y-%m-%d')
    weekday = today.strftime('%A')
    swedish_weekday = SWEDISH_WEEKDAYS.get(weekday, weekday)
    swedish_month = SWEDISH_MONTHS.get(today.month, today.strftime('%B').lower())
    date_context = f"{swedish_weekday} den {today.day} {swedish_month} {today.year}"
    
    # L√§s tidigare anv√§nda artiklar f√∂r upprepningsfilter (senaste 21 dagarna)
    # (viktigt f√∂r att undvika att samma nyhet tas upp dag efter dag)
    dedupe_days = 21
    used_articles = set()
    recent_episode_articles: List[Dict[str, Any]] = []

    memory_days_env = os.getenv('MMM_MEMORY_DAYS', '').strip()
    try:
        memory_days = int(memory_days_env) if memory_days_env else 60
    except ValueError:
        memory_days = 60
    memory_days = max(memory_days, dedupe_days)

    # Prim√§rt: anv√§nd en persistent historikfil (fungerar i GitHub Actions via cache)
    history = None
    try:
        from src.news_dedupe import NewsHistory

        history = NewsHistory("news_history.json")
        history.load()
        history.prune(keep_days=60)
        logger.info("[HISTORY] news_history.json loaded (%s keys)", len(history.data.get('items', {})))
    except Exception as e:
        history = None
        logger.warning(f"[HISTORY] Kunde inte initiera news_history.json: {e}")
    try:
        recent_episode_articles = _load_recent_episode_articles(within_days=memory_days, today=today)
        cutoff_date = (today - timedelta(days=dedupe_days)).date()
        for prev in recent_episode_articles:
            try:
                prev_dt = prev.get('date')
                if isinstance(prev_dt, datetime) and prev_dt.date() < cutoff_date:
                    continue
            except Exception:
                continue

            prev_link = prev.get('link', '')
            prev_title = prev.get('title', '')
            url_key = _canonicalize_url(prev_link)
            fp_key = _title_fingerprint(prev_title)
            if url_key:
                used_articles.add(url_key)
            if fp_key:
                used_articles.add(fp_key)

        logger.info(f"[HISTORY] Laddade {len(used_articles)} tidigare anv√§nda artiklar f√∂r upprepningsfilter")
    except Exception as e:
        logger.warning(f"[HISTORY] Upprepningsfilter misslyckades: {e}")
    
    # ============================================================
    # MULTI-AGENT NEWS CURATION SYSTEM
    # ============================================================
    logger.info("\n" + "="*80)
    logger.info("ü§ñ AGENT-BASERAD NYHETSKURERING STARTAR")
    logger.info("="*80)
    
    # Importera agent-systemet
    available_articles: List[Dict] = []
    try:
        from news_curation_integration import curate_news_sync

        # Anv√§nd agent-systemet f√∂r att kurera artiklar
        available_articles = curate_news_sync('scraped_content.json')

        # Viktigt: agent-systemet kan "lyckas" men √§nd√• ge 0 artiklar. D√• f√•r vi
        # ett extremt kort avsnitt (ex. 1 minut). Falla tillbaka till enkel filtrering.
        if not available_articles:
            logger.warning("‚ö†Ô∏è Agent-systemet returnerade 0 artiklar. Faller tillbaka p√• enkel filtrering...")
            available_articles = _fallback_collect_articles_from_scraped()

        logger.info(f"\n‚úÖ Artikelurval klart: {len(available_articles)} artiklar f√∂r podcast")
        logger.info("="*80 + "\n")

    except Exception as e:
        logger.error(f"‚ùå Agent-systemet misslyckades: {e}")
        logger.warning("Faller tillbaka p√• enkel filtrering...")
        available_articles = _fallback_collect_articles_from_scraped()
    
    # Skapa artikelreferenser f√∂r AI
    article_refs = ""
    if available_articles:
        # Filtrera bort upprepningar (men till√•t uppf√∂ljningar)
        filtered_articles = []
        skipped_count = 0
        skipped_articles: List[Dict] = []
        for a in available_articles:
            url_key = _canonicalize_url(a.get('link', ''))
            fp_key = _title_fingerprint(a.get('title', ''))

            # Matcha b√•de "legacy" (episode_articles_*.json) och persistent historik (news_history.json)
            hist_url_key = f"url:{url_key}" if url_key else ""
            hist_fp_key = f"title:{fp_key}" if fp_key else ""

            is_repeat_by_url = bool(url_key and url_key in used_articles)
            is_repeat_by_fp = bool(fp_key and fp_key in used_articles)

            if history is not None:
                try:
                    is_repeat_by_url = is_repeat_by_url or bool(hist_url_key and history.seen_within_days(hist_url_key, dedupe_days))
                    is_repeat_by_fp = is_repeat_by_fp or bool(hist_fp_key and history.seen_within_days(hist_fp_key, dedupe_days))
                except Exception:
                    # Historik ska aldrig stoppa k√∂rningen
                    pass

            is_repeat = is_repeat_by_url or is_repeat_by_fp
            is_follow_up = _is_follow_up_article(a.get('title', ''), a.get('content', ''))

            if is_repeat and not is_follow_up:
                skipped_count += 1
                skipped_articles.append(a)
                logger.info(f"[HISTORY] Skipping repeat: {a.get('source', '')} - {a.get('title', '')[:80]}")
                log_diagnostic('article_skipped_repeat', {
                    'source': a.get('source', ''),
                    'title': a.get('title', ''),
                    'link': a.get('link', ''),
                    'repeat_by_url': is_repeat_by_url,
                    'repeat_by_title_fingerprint': is_repeat_by_fp,
                    'url_key': url_key,
                    'title_fingerprint': fp_key,
                })
                continue

            if is_repeat and is_follow_up:
                log_diagnostic('article_allowed_follow_up', {
                    'source': a.get('source', ''),
                    'title': a.get('title', ''),
                    'link': a.get('link', ''),
                    'repeat_by_url': is_repeat_by_url,
                    'repeat_by_title_fingerprint': is_repeat_by_fp,
                })

            filtered_articles.append(a)

            # Markera som sedd i persistent historik n√§r vi v√§ljer att beh√•lla artikeln
            if history is not None:
                try:
                    history.mark_seen([hist_url_key, hist_fp_key])
                except Exception:
                    pass

        if skipped_count:
            logger.info(f"[HISTORY] Filtrerade bort {skipped_count} upprepade artiklar")
            log_diagnostic('dedupe_summary', {
                'skipped_repeat_count': skipped_count,
                'remaining_article_count': len(filtered_articles),
            })

        # Om dedupe blir f√∂r aggressiv: fyll p√• med repeats s√• vi kan skapa ett full√§ngdsavsnitt.
        # Hellre lite repetition √§n att publicera ~1 minut.
        min_articles_env = os.getenv('MMM_MIN_ARTICLES', '').strip()
        try:
            min_articles = int(min_articles_env) if min_articles_env else 6
        except ValueError:
            min_articles = 6

        if len(filtered_articles) < min_articles and skipped_articles:
            needed = min_articles - len(filtered_articles)
            logger.warning(f"[HISTORY] F√∂r f√• artiklar efter dedupe ({len(filtered_articles)}<{min_articles}). √Öterinf√∂r {needed} repeats f√∂r att n√• miniminiv√•.")
            log_diagnostic('dedupe_relaxed', {
                'min_articles': min_articles,
                'before_count': len(filtered_articles),
                'skipped_available': len(skipped_articles),
            })
            filtered_articles.extend(skipped_articles[:needed])
            log_diagnostic('dedupe_relaxed', {
                'after_count': len(filtered_articles),
            })

        available_articles = filtered_articles

        # Spara persistent historik (om den √§r aktiverad)
        if history is not None:
            try:
                history.save()
                logger.info("[HISTORY] news_history.json saved")
            except Exception as e:
                logger.warning(f"[HISTORY] Kunde inte spara news_history.json: {e}")

        article_refs = "\n\nTILLG√ÑNGLIGA ARTIKLAR ATT REFERERA TILL:\n"
        max_article_chars_env = os.getenv('MMM_PROMPT_ARTICLE_CHARS', '').strip()
        try:
            max_article_chars = int(max_article_chars_env) if max_article_chars_env else 650
        except ValueError:
            max_article_chars = 650

        for i, article in enumerate(available_articles[:10], 1):
            article_title = _truncate_text(article.get('title', ''), 140)
            article_content = _truncate_text(article.get('content', ''), max_article_chars)
            article_source = article.get('source', 'Ok√§nd k√§lla')
            article_refs += f"{i}. {article_source}: {article_title}\n   Inneh√•ll: {article_content}\n   [Referera som: {article_source}]\n\n"

    callback_refs = ""
    try:
        max_callbacks_env = os.getenv('MMM_MEMORY_MAX_CALLBACKS', '').strip()
        try:
            max_callbacks = int(max_callbacks_env) if max_callbacks_env else 3
        except ValueError:
            max_callbacks = 3
        max_callbacks = max(0, min(6, max_callbacks))

        if max_callbacks and recent_episode_articles:
            min_overlap_env = os.getenv('MMM_MEMORY_MIN_OVERLAP', '').strip()
            min_jaccard_env = os.getenv('MMM_MEMORY_MIN_JACCARD', '').strip()
            max_freq_env = os.getenv('MMM_MEMORY_ANCHOR_MAX_FREQ', '').strip()
            min_len_env = os.getenv('MMM_MEMORY_ANCHOR_MIN_LEN', '').strip()
            try:
                min_overlap = int(min_overlap_env) if min_overlap_env else 4
            except ValueError:
                min_overlap = 4
            try:
                min_jaccard = float(min_jaccard_env) if min_jaccard_env else 0.50
            except ValueError:
                min_jaccard = 0.50
            try:
                anchor_max_freq = int(max_freq_env) if max_freq_env else 2
            except ValueError:
                anchor_max_freq = 2
            try:
                anchor_min_len = int(min_len_env) if min_len_env else 7
            except ValueError:
                anchor_min_len = 7

            debug_env = os.getenv('MMM_MEMORY_DEBUG', '').strip().lower()
            memory_debug = debug_env in {'1', 'true', 'yes', 'on'}
            debug_samples_env = os.getenv('MMM_MEMORY_DEBUG_MAX_SAMPLES', '').strip()
            try:
                debug_max_samples = int(debug_samples_env) if debug_samples_env else 6
            except ValueError:
                debug_max_samples = 6

            hints, debug_info = _build_previous_coverage_hints(
                current_articles=available_articles[:10],
                recent_episode_articles=recent_episode_articles,
                today=today,
                max_hints=max_callbacks,
                min_overlap_tokens=max(2, min_overlap),
                min_jaccard=max(0.10, min(0.90, min_jaccard)),
                informative_max_freq=max(1, anchor_max_freq),
                min_anchor_token_len=max(4, anchor_min_len),
                debug=memory_debug,
                debug_max_samples=max(0, debug_max_samples),
            )

            # Always log a compact summary so we can tune thresholds without guesswork.
            try:
                log_diagnostic('memory_match_summary', {
                    'window_days': memory_days,
                    'max_callbacks': max_callbacks,
                    'min_overlap': min_overlap,
                    'min_jaccard': min_jaccard,
                    'anchor_max_freq': anchor_max_freq,
                    'anchor_min_len': anchor_min_len,
                    'current_considered': int((debug_info or {}).get('current_considered', 0)),
                    'recent_items': int((debug_info or {}).get('recent_items', 0)),
                    'evaluated_pairs': int((debug_info or {}).get('evaluated_pairs', 0)),
                    'reasons': (debug_info or {}).get('reasons', {}),
                    'samples': (debug_info or {}).get('samples', []) if memory_debug else [],
                })
            except Exception:
                pass

            # Optional: print to main log (Actions) for easier tuning.
            if memory_debug:
                try:
                    reasons = (debug_info or {}).get('reasons', {}) or {}
                    logger.info(
                        "[MEMORY][DEBUG] Summary: considered=%s recent=%s pairs=%s emitted=%s no_candidate=%s no_anchor=%s overlap_below=%s sim_below=%s",
                        (debug_info or {}).get('current_considered', 0),
                        (debug_info or {}).get('recent_items', 0),
                        (debug_info or {}).get('evaluated_pairs', 0),
                        reasons.get('hint_emitted', 0),
                        reasons.get('no_candidate', 0),
                        reasons.get('no_anchor_tokens', 0),
                        reasons.get('overlap_below_min', 0),
                        reasons.get('similarity_below_min', 0),
                    )
                    samples = (debug_info or {}).get('samples', []) or []
                    for s in samples[:max(0, debug_max_samples)]:
                        logger.info(
                            "[MEMORY][DEBUG] Sample: '%s' (%s) anchors='%s' best_score=%s prev=%s %s",
                            s.get('current_title', ''),
                            s.get('current_source', ''),
                            s.get('anchor_tokens', ''),
                            s.get('best_any_score', ''),
                            s.get('best_any_prev_date', ''),
                            s.get('best_any_prev_source', ''),
                        )
                except Exception:
                    pass

            if hints:
                callback_refs = "\n\nMINNESKROKAR (INTERNT UNDERLAG ‚Äì SKA INTE √ÖTERGES SOM LISTA I MANUSET):\n"
                callback_refs += (
                    "- Endast om du √§r MYCKET s√§ker p√• att det √§r samma √§mne: l√§gg in EN naturlig mening i f√∂rbifarten, "
                    "t.ex. 'Vi var inne p√• det h√§r √§mnet senast den ...'.\n"
                    "- Om du √§r det minsta os√§ker: hoppa √∂ver √•terkopplingen helt.\n"
                    "- S√§g '√§mnet' (inte 'exakt samma nyhet') och n√§mn g√§rna datum, men undvik att √•terge den gamla rubriken.\n\n"
                )
                for h in hints:
                    topic = (h.get('topic_tokens') or '').strip()
                    topic_part = f" (√§mnesord: {topic})" if topic else ""
                    callback_refs += (
                        f"MINNESKROK: Dagens k√§lla {h.get('current_source','')}{topic_part}. "
                        f"Senast vi var inne p√• √§mnet: {h.get('previous_date','')} (k√§lla: {h.get('previous_source','')}).\n"
                    )

                log_diagnostic('memory_callbacks_built', {
                    'count': len(hints),
                    'window_days': memory_days,
                    'min_overlap': min_overlap,
                    'min_jaccard': min_jaccard,
                    'anchor_max_freq': anchor_max_freq,
                    'anchor_min_len': anchor_min_len,
                })
            else:
                log_diagnostic('memory_callbacks_none', {
                    'window_days': memory_days,
                    'min_overlap': min_overlap,
                    'min_jaccard': min_jaccard,
                    'anchor_max_freq': anchor_max_freq,
                    'anchor_min_len': anchor_min_len,
                })
    except Exception as e:
        logger.warning(f"[MEMORY] Kunde inte bygga √•terkopplingar: {e}")

    # Absolut krav: om vi inte har n√•gon artikel att basera avsnittet p√• ska vi inte f√∂rs√∂ka.
    # Detta tenderar annars att ge superkorta/manusl√∂sa avsnitt.
    min_articles_env = os.getenv('MMM_MIN_ARTICLES', '').strip()
    try:
        min_articles = int(min_articles_env) if min_articles_env else 6
    except ValueError:
        min_articles = 6
    if not available_articles or len(available_articles) < 1:
        raise RuntimeError("Inga artiklar tillg√§ngliga efter kurering/filtrering; avbryter f√∂r att undvika kort/ok√§llat avsnitt")
    
    aftertalk_instructions = ""
    if aftertalk_cfg:
        style = aftertalk_cfg.get('style', '')
        style_block = f"\nSTIL (eftersnack): {style}\n" if style else ""
        aftertalk_instructions = (
            "\n\nEFTERSNACK (ENBART IDAG - KRITISKT):\n"
            f"- L√§gg till ett kort EFTERSNACK precis i slutet, efter ordinarie outro (eller som en tydligt separerad bonus p√• slutet).\n"
            f"- L√§ngd: cirka {aftertalk_cfg.get('target_seconds', 120)} sekunder (till√•tet spann {aftertalk_cfg.get('min_seconds', 90)}‚Äì{aftertalk_cfg.get('max_seconds', 150)} sek).\n"
            "- Inneh√•ll: meta, l√§tt komiskt, sj√§lvdistanserat bakom-kulisserna-snack om hur dagens manus byggdes, utan nya fakta eller nya nyheter.\n"
            "- INGA nya k√§llor, inga nya p√•st√•enden, inga nya rubriker. Bara reflektion/ton.\n"
            + style_block
        )
    else:
        aftertalk_instructions = (
            "\n\nEFTERSNACK (F√ñRBJUDET - KRITISKT):\n"
            "- L√§gg INTE till eftersnack/efterprat/bonussegment.\n"
            "- Avsluta n√§r outrot √§r klart, och skriv inga extra repliker efter sista avslutningen.\n"
        )

    prompt = f"""Skapa ett KOMPLETT och DETALJERAT manus f√∂r dagens avsnitt av "MMM Senaste Nytt" - en svensk daglig nyhetspodcast om teknik, AI och klimat.

DATUM (KRITISKT): {date_str} ({date_context})
V√ÑDER: {weather_info}
L√ÑNGD: Absolut m√•l √§r 10 minuter (minst 1800-2200 ord f√∂r talat inneh√•ll)
V√ÑRDAR: Lisa (kvinnlig, professionell men v√§nlig) och Pelle (manlig, nyfiken och engagerad)

{article_refs}{callback_refs}

DETALJERAD STRUKTUR:
1. INTRO & V√ÑLKOMST (90-120 sekunder) - Inkludera RIKTIG v√§derinfo fr√•n "{weather_info}"
2. INNEH√ÖLLS√ñVERSIKT (60-90 sekunder) - Detaljerad genomg√•ng av alla √§mnen
3. HUVUDNYHETER (6-7 minuter) - 5-6 nyheter med djup analys och dialog  
4. DJUP DISKUSSION/ANALYS (2-3 minuter) - Lisa och Pelle diskuterar trender och framtid
5. SAMMANFATTNING (60-90 sekunder) - Detaljerad recap av alla √§mnen
6. OUTRO & MMM-KOPPLING (60-90 sekunder) - STARK koppling till huvudpodden "M√§nniska Maskin Milj√∂"

INNEH√ÖLLSKRAV OCH √ÑMNESF√ñRDELNING:
- KRITISKT: Utg√• fr√•n DATUM (KRITISKT) ovan (= dagens datum f√∂r denna k√∂rning). N√§mn inte fel m√•nad (t.ex. "november") eller fel datum.
- Lisa s√§ger "MMM Senaste Nytt" naturligt och professionellt (inte √∂verdrivet)
- Anv√§nd RIKTIG v√§derdata: "{weather_info}" - inte p√•hittade kommentarer om "fin dag i Stockholm"
- Minst 6 konkreta nyheter fr√•n svenska och internationella k√§llor
- OBLIGATORISK √ÑMNESF√ñRDELNING (mycket viktigt f√∂r balans):
  * MINST 50% av nyheterna M√ÖSTE handla om KLIMAT, MILJ√ñ och H√ÖLLBARHET
  * Maximalt 50% f√•r handla om AI och teknologi (inte klimatrelaterad)
  * Prioritera SVENSKA klimat- och milj√∂nyheter n√§r de finns tillg√§ngliga
  * Exempel: Om du har 6 nyheter, MINST 3 ska vara klimat/milj√∂
- Varje nyhet ska vara minst 150-200 ord inklusive diskussion
- Lisa och Pelle ska ha naturliga konversationer med f√∂ljdfr√•gor
- Inkludera siffror, fakta och konkreta exempel
- N√§mn specifika f√∂retag, forskare eller organisationer

REDATIONELL LINJE (KRITISKT):
- MMM Senaste Nytt √§r INTE f√∂r k√§rnkraft som klimatl√∂sning. Presentera inte k√§rnkraft som "l√∂sningen".
- Om k√§rnkraft n√§mns: var saklig och kritisk (mycket h√∂ga kostnader, l√•nga byggtider som f√∂rsenar fossil undantr√§ngning,
  avfallsfr√•gan/slutf√∂rvar i mycket l√•ng tid, och att det riskerar tr√§nga undan snabbare alternativ).

K√ÑLLH√ÑNVISNING - MYCKET VIKTIGT:
- ANV√ÑND ENDAST artiklarna listade ovan som k√§llor f√∂r dina nyheter - ALDRIG sociala medier eller op√•litliga k√§llor
- VARJE nyhet M√ÖSTE baseras p√• en specifik artikel fr√•n seri√∂sa medier (SVT, DN, BBC, Reuters etc.)
- F√ñRBJUDET: Facebook, Twitter/X, Instagram, TikTok, YouTube eller andra sociala medier som k√§llor
- Referera tydligt: "SVT Nyheter rapporterar att...", "BBC News skriver att...", "Reuters meddelar att..."
- Specifika personer M√ÖSTE namnges n√§r de finns i artiklarna (t.ex. "Milj√∂minister Romina Pourmokhtari s√§ger...", "Enligt statsminister Ulf kristersson...")
- Konkreta detaljer M√ÖSTE tas fr√•n artiklarna - p√•hitta ALDRIG fakta
- N√§r m√∂jligt: anv√§nd siffror och fakta fr√•n artiklarna
- Om information saknas i artiklarna - s√§g det tydligt ("detaljerna √§r √§nnu inte k√§nda", "ingen tidsplan har presenterats")
- Undvik vaga termer - var specifik baserat p√• vad som faktiskt st√•r i artiklarna
- Lyssnarna ska kunna f√∂rst√• att nyheten kommer fr√•n en etablerad, trov√§rdig nyhetsk√§lla

OUTRO-KRAV (MYCKET VIKTIGT):
- INGEN teasing av "n√§sta avsnitt" 
- INGA p√•hittade lyssnarfr√•gor
- STARK koppling till huvudpodden "M√§nniska Maskin Milj√∂"
- F√∂rklara att MMM Senaste Nytt √§r en del av M√§nniska Maskin Milj√∂-familjen
- Uppmana lyssnare att kolla in huvudpodden f√∂r djupare analyser
- OBLIGATORISK AI-BRASKLAPP: Lisa och Pelle ska √∂dmjukt f√∂rklara att de √§r AI-r√∂ster och att information kan inneh√•lla fel, h√§nvisa till l√§nkarna i avsnittsinformationen f√∂r verifiering

SLUTREGLER (KRITISKT):
- Ingen utfyllnad efter outrot (om inte EFTERSNACK √§r explicit till√•tet idag).
- Ingen "vi forts√§tter", "en sista grej", "bonus" eller liknande efter att ni sagt tack och rundat av.
{aftertalk_instructions}

DIALOGREGLER:
- Anv√§nd naturliga √∂verg√•ngar: "Det p√•minner mig om...", "Aprop√• det...", "Interessant nog..."
- Lisa och Pelle ska ibland avbryta varandra naturligt
- Inkludera korta pauser f√∂r eftertanke: "Hmm, det √§r en bra po√§ng..."
- Anv√§nd svenska uttryck och vardagligt spr√•k
- Variera meningsl√§ngderna f√∂r naturligt flyt
- KRITISKT: Varje replik m√•ste logiskt f√∂lja f√∂reg√•ende - ingen f√•r svara p√• n√•got som inte sagts √§nnu
- KRITISKT: Om n√•gon n√§mner en siffra/statistik, m√•ste den f√∂rst ha presenterats i tidigare replik
- Kontrollera att alla h√§nvisningar ("det", "den siffran", "som du sa") faktiskt refererar till n√•got som redan sagts

FORMAT: Skriv ENDAST som ren dialog med "Talarnamn: Text" - INGEN markdown eller formatering!
F√ñRBJUDET: **, ##, ---, ###, rubriker, markeringar, lyssnarfr√•gor, "n√§sta avsnitt" - bara ren dialog!

EXEMPEL INTRO:
Lisa: Hej och v√§lkommen till MMM Senaste Nytt! Jag heter Lisa.
Pelle: Och jag heter Pelle. Idag √§r det {swedish_weekday} den {today.day} {swedish_month} {today.year}, och {weather_info.lower()}.
Lisa: Ja, det st√§mmer! Men vi har mycket sp√§nnande att prata om idag inom teknik, AI och klimat.

VIKTIGT: Endast dialog - inga rubriker eller formatering! Bara "Namn: Text" rad f√∂r rad.

Skapa nu ett KOMPLETT och L√ÖNGT manus f√∂r dagens avsnitt - kom ih√•g minst 1800 ord:"""

    messages = [{"role": "user", "content": prompt}]
    
    def generate_fallback_content_from_articles() -> str:
        # Bygg ett l√§ngre, k√§llbaserat manus utan LLM s√• att vi inte publicerar ~1 minut.
        chosen = list(available_articles or [])
        if not chosen:
            chosen = _fallback_collect_articles_from_scraped(max_per_source=4, max_total=20)

        # F√∂rs√∂k f√• minst 8 nyheter. Om vi inte har det, ta det vi har.
        chosen = chosen[:10]

        intro = (
            f"Lisa: Hej och v√§lkommen till MMM Senaste Nytt! Jag heter Lisa.\n\n"
            f"Pelle: Och jag heter Pelle. Idag √§r det {swedish_weekday} den {today.day} {swedish_month} {today.year}, och {weather_info.lower()}.\n\n"
            "Lisa: Vi g√•r igenom dagens viktigaste nyheter inom teknik, AI, klimat och milj√∂ ‚Äì och vi ber√§ttar tydligt vilka k√§llor vi bygger p√•.\n\n"
            "Pelle: Bra! Vi k√∂r ig√•ng.\n"
        )

        overview_parts = []
        for a in chosen[:8]:
            overview_parts.append(f"{a.get('source','')}: {a.get('title','')}")
        overview = (
            "\nLisa: H√§r √§r en snabb √∂versikt p√• vad vi tar upp idag.\n"
            f"Pelle: { ' | '.join([p for p in overview_parts if p]) }.\n"
        )

        body_lines: List[str] = []
        for idx, a in enumerate(chosen[:8], 1):
            source = a.get('source', 'Ok√§nd k√§lla')
            title = (a.get('title') or '').strip()
            link = (a.get('link') or '').strip()
            content_snip = _truncate_text(a.get('content', ''), 1100)
            if not content_snip:
                content_snip = "Detaljerna √§r begr√§nsade i v√•rt underlag just nu, men rubriken ger en tydlig signal om vad som h√§nt."

            body_lines.append(f"\nLisa: Nyhet {idx}. {source} rapporterar i artikeln \"{title}\".")
            body_lines.append(f"Pelle: Okej, vad √§r k√§rnan h√§r ‚Äì och vad vet vi faktiskt?")
            body_lines.append(
                "Lisa: "
                + content_snip
                + ("" if content_snip.endswith(('.', '!', '?')) else ".")
            )
            body_lines.append(
                "Pelle: Det h√§r v√§cker ju fr√•gan om konsekvenser och n√§sta steg. Finns det n√•got som fortfarande √§r oklart?"
            )
            body_lines.append(
                "Lisa: Ja ‚Äì och det √§r viktigt att s√§ga h√∂gt: om k√§lltexten inte ger exakta siffror eller tidsplaner s√• l√•tsas vi inte. Vi f√∂ljer upp n√§r mer information finns."
            )
            body_lines.append(
                "Pelle: Och som alltid: vi l√§nkar till originalk√§llan s√• att du kan l√§sa mer sj√§lv och bed√∂ma detaljerna."
            )
            if link:
                body_lines.append(f"Pelle: L√§nk finns i avsnittsbeskrivningen ‚Äì k√§lla: {source}.")

        outro = (
            "\nLisa: Det var dagens genomg√•ng. Kom ih√•g: vi √§r AI-r√∂ster och vi kan g√∂ra fel, s√• dubbelkolla g√§rna via l√§nkarna i avsnittsinformationen.\n"
            "Pelle: Och vill du ha mer djup och sammanhang s√• finns huvudpodden M√§nniska Maskin Milj√∂, d√§r vi g√•r l√§ngre i analysen.\n"
            "Lisa: Tack f√∂r att du lyssnade p√• MMM Senaste Nytt!\n"
        )

        draft = (intro + "\n" + overview + "\n" + "\n".join(body_lines) + "\n" + outro).strip()

        # S√§kerst√§ll minl√§ngd √§ven i fallback-l√§get.
        min_words_env_local = os.getenv('MMM_MIN_SCRIPT_WORDS', '').strip()
        try:
            min_words_local = int(min_words_env_local) if min_words_env_local else 1400
        except ValueError:
            min_words_local = 1400
        if _should_pad_short_scripts():
            return _append_article_padding(draft, chosen, min_words_local)
        return draft

    try:
        content = get_openrouter_response(messages)
        # Guard: ibland svarar modellen alldeles f√∂r kort (t.ex. n√§r k√§llistan √§r tunn).
        # G√∂r en explicit retry med skarpare instruktioner, annars riskerar vi 1-minutsavsnitt.
        min_words_env = os.getenv('MMM_MIN_SCRIPT_WORDS', '').strip()
        try:
            min_words = int(min_words_env) if min_words_env else 1400
        except ValueError:
            min_words = 1400
        wc = _count_words(content)
        if wc < min_words:
            logger.warning(f"[AI] Manus f√∂r kort ({wc} ord < {min_words}). F√∂rs√∂ker en g√•ng till med f√∂rtydligad prompt...")
            log_diagnostic('ai_script_too_short_retry', {
                'word_count': wc,
                'min_words': min_words,
                'model': 'openai/gpt-4o-mini',
            })
            retry_prompt = prompt + "\n\nVIKTIGT: Ditt f√∂rra svar blev f√∂r kort. Skriv om manuset till minst " + str(min_words) + " ord. Beh√•ll exakt samma FORMAT (bara 'Namn: text') och anv√§nd de listade k√§llorna."
            content = get_openrouter_response([{"role": "user", "content": retry_prompt}])
            wc2 = _count_words(content)
            logger.info(f"[AI] Retry word count: {wc2}")
            if wc2 < min_words:
                if _should_pad_short_scripts():
                    logger.warning(
                        f"[AI] Retry fortfarande kort ({wc2} < {min_words}). Pad:ar upp med rubrikrunda (MMM_PAD_SHORT_SCRIPTS=1)."
                    )
                    log_diagnostic('ai_script_padded', {
                        'word_count_before': wc2,
                        'min_words': min_words,
                    })
                    content = _append_article_padding(content, available_articles, min_words)
                else:
                    logger.warning(
                        f"[AI] Retry fortfarande kort ({wc2} < {min_words}). L√§mnar manuset kort (MMM_PAD_SHORT_SCRIPTS=0)."
                    )
        logger.info("[AI] Genererade podcast-inneh√•ll med v√§derdata")
        return content, available_articles
    except Exception as e:
        logger.error(f"[ERROR] Kunde inte generera AI-inneh√•ll: {e}")
        log_diagnostic('ai_script_generation_failed', {
            'error': f"{type(e).__name__}: {e}",
            'fallback': 'article_based',
        })
        # Fallback till k√§llbaserat (l√§ngre) inneh√•ll
        return generate_fallback_content_from_articles(), available_articles

def generate_fallback_content(date_str: str, weekday: str, weather_info: str) -> str:
    """Fallback-inneh√•ll om AI inte fungerar"""
    try:
        dt = datetime.strptime(date_str, '%Y-%m-%d')
        month_swedish = SWEDISH_MONTHS.get(dt.month, dt.strftime('%B').lower())
        day = dt.day
        year = dt.year
    except Exception:
        month_swedish = datetime.now().strftime('%B').lower()
        day = int(datetime.now().strftime('%d'))
        year = datetime.now().year

    return f"""Lisa: Hej och v√§lkommen till MMM Senaste Nytt! Jag heter Lisa.

Pelle: Och jag heter Pelle. Idag √§r det {weekday} den {day} {month_swedish} {year}, och {weather_info.lower()}.

Lisa: Precis! Vi har mycket att prata om idag inom teknik, AI och klimat.

Pelle: Vi b√∂rjar med att OpenAI har lanserat en f√∂rb√§ttrad version av GPT-4 som de kallar GPT-4 Turbo. Modellen √§r inte bara snabbare, utan ocks√• betydligt billigare att anv√§nda f√∂r utvecklare.

Lisa: Det √§r verkligen stora nyheter, Pelle. Vad tror du det betyder f√∂r svenska f√∂retag som anv√§nder AI?

Pelle: Jag tror det kommer att demokratisera AI-anv√§ndningen. Mindre f√∂retag kommer nu ha r√•d att bygga avancerade AI-l√∂sningar.

Lisa: Sp√§nnande! Vi forts√§tter med klimatnyheter. Tesla har presenterat sina nya 4680-batterier som p√•st√•s ha 50% b√§ttre energidensitet.

Pelle: Det √§r riktigt intressanta nyheter. B√§ttre batterier √§r nyckeln till b√•de elbilar och energilagring.

Lisa: F√∂r att sammanfatta har vi pratat om OpenAI:s nya modell och Teslas batteriteknik.

Pelle: Det h√§r avsnittet av MMM Senaste Nytt √§r en del av v√•rt st√∂rre program M√§nniska Maskin Milj√∂, d√§r vi g√•r djupare in p√• hur teknik, klimat och samh√§lle p√•verkar varandra.

Lisa: Tack f√∂r att ni lyssnade! Vi h√∂rs imorgon med fler nyheter."""

def clean_text_for_tts(text: str) -> str:
    """Ta bort markdown-formattering och andra tecken som inte ska l√§sas upp"""
    # Ta bort markdown-formattering
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # **text** -> text
    text = re.sub(r'\*([^*]+)\*', r'\1', text)      # *text* -> text
    text = re.sub(r'#{1,6}\s*', '', text)           # ### headings -> 
    text = re.sub(r'---+', '', text)                # --- -> 
    text = re.sub(r'^\s*[-*+]\s*', '', text, flags=re.MULTILINE)  # list items
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # [text](link) -> text
    
    # Ta bort andra formattering
    text = re.sub(r'`([^`]+)`', r'\1', text)        # `code` -> code
    text = re.sub(r'_{1,2}([^_]+)_{1,2}', r'\1', text)  # __text__ -> text
    
    # Rensa extra whitespace
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def split_long_text_for_tts(text: str, speaker: str, max_bytes: int = 4000) -> List[Dict]:
    """Dela upp l√•ng text i TTS-kompatibla segment"""
    segments = []
    
    # Om texten √§r kort nog, returnera som ett segment
    if len(text.encode('utf-8')) <= max_bytes:
        return [{'speaker': speaker, 'text': text}]
    
    # Dela vid meningar f√∂rst
    sentences = re.split(r'(?<=[.!?])\s+', text)
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence_bytes = len(sentence.encode('utf-8'))
        
        # Om en enskild mening √§r f√∂r l√•ng, dela den h√•rdare
        if sentence_bytes > max_bytes:
            # Spara nuvarande chunk f√∂rst
            if current_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(current_chunk).strip()
                })
                current_chunk = []
                current_size = 0
            
            # Dela den l√•nga meningen vid komma eller ord
            words = sentence.split()
            word_chunk = []
            word_size = 0
            
            for word in words:
                word_bytes = len((word + ' ').encode('utf-8'))
                if word_size + word_bytes > max_bytes and word_chunk:
                    segments.append({
                        'speaker': speaker,
                        'text': ' '.join(word_chunk).strip()
                    })
                    word_chunk = [word]
                    word_size = word_bytes
                else:
                    word_chunk.append(word)
                    word_size += word_bytes
            
            if word_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(word_chunk).strip()
                })
        else:
            # Kontrollera om vi kan l√§gga till meningen
            if current_size + sentence_bytes > max_bytes and current_chunk:
                segments.append({
                    'speaker': speaker,
                    'text': ' '.join(current_chunk).strip()
                })
                current_chunk = [sentence]
                current_size = sentence_bytes
            else:
                current_chunk.append(sentence)
                current_size += sentence_bytes
    
    # L√§gg till sista chunk
    if current_chunk:
        segments.append({
            'speaker': speaker,
            'text': ' '.join(current_chunk).strip()
        })
    
    return segments

def parse_podcast_text(text: str) -> List[Dict]:
    """Parsa podcast-text i segment med talare och repliker"""
    segments = []
    lines = text.strip().split('\n')
    
    current_speaker = None
    current_text = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Hoppa √∂ver rena formattering-rader
        if line.startswith('#') or line.startswith('---') or line.startswith('### '):
            continue
            
        # Identifiera talare-rader (med eller utan markdown-formattering)
        if ':' in line and len(line.split(':', 1)) == 2:
            speaker_part, text_part = line.split(':', 1)
            
            # Rensa speaker-namnet fr√•n formattering
            speaker_name = clean_text_for_tts(speaker_part).strip()
            
            # Hoppa √∂ver om det inte ser ut som ett talare-namn
            if not speaker_name or len(speaker_name.split()) > 2:
                if current_speaker:
                    current_text.append(clean_text_for_tts(line))
                continue
            
            # Spara f√∂reg√•ende segment
            if current_speaker and current_text:
                clean_text = ' '.join(current_text).strip()
                if clean_text:  # Endast om det finns text
                    # Dela upp l√•nga segment f√∂r TTS-kompatibilitet (max 4000 bytes)
                    split_segments = split_long_text_for_tts(clean_text, current_speaker)
                    segments.extend(split_segments)
            
            # Starta nytt segment
            current_speaker = speaker_name
            text_part_clean = clean_text_for_tts(text_part).strip()
            current_text = [text_part_clean] if text_part_clean else []
        else:
            # Forts√§tt med samma talare
            if current_speaker:
                clean_line = clean_text_for_tts(line)
                if clean_line:  # Endast l√§gg till om det finns text
                    current_text.append(clean_line)
    
    # L√§gg till sista segmentet
    if current_speaker and current_text:
        clean_text = ' '.join(current_text).strip()
        if clean_text:  # Endast om det finns text
            # Dela upp l√•nga segment f√∂r TTS-kompatibilitet (max 4000 bytes)
            split_segments = split_long_text_for_tts(clean_text, current_speaker)
            segments.extend(split_segments)
    
    return segments

def generate_audio_with_gemini_dialog(script_content: str, weather_info: str, output_file: str) -> bool:
    """Generera audio med Gemini TTS f√∂r naturlig dialog mellan Lisa och Pelle"""
    if not GEMINI_TTS_AVAILABLE:
        logger.info("[AUDIO] Gemini TTS inte tillg√§nglig, anv√§nder standard-metod")
        _set_last_gemini_error("GEMINI_TTS_AVAILABLE=False (importfel eller saknade dependencies)")
        return False
    
    try:
        logger.info("[AUDIO] Genererar naturlig dialog med Gemini TTS...")
        
        generator = GeminiTTSDialogGenerator()
        
        # Om manus redan √§r en dialog (Lisa:/Pelle:), anv√§nd det i st√§llet f√∂r att
        # bygga om via heuristisk split (som kan skapa rader utan talarprefix).
        if any(tag in script_content for tag in ("Lisa:", "Pelle:", "LISA:", "PELLE:")):
            parsed_segments = parse_podcast_text(script_content)

            dialog_lines = []
            fallback_speaker = "Lisa"
            for seg in parsed_segments:
                speaker_raw = (seg.get('speaker') or '').strip()
                text = (seg.get('text') or '').strip()
                if not text:
                    continue

                speaker_lower = speaker_raw.lower()
                if 'lisa' in speaker_lower:
                    speaker = "Lisa"
                elif 'pelle' in speaker_lower:
                    speaker = "Pelle"
                else:
                    # Skulle bara intr√§ffa om manus inneh√•ller annan talare.
                    speaker = fallback_speaker
                    fallback_speaker = "Pelle" if fallback_speaker == "Lisa" else "Lisa"

                dialog_lines.append(f"{speaker}: {text}")

            dialog_script = "\n".join(dialog_lines).strip()
            logger.info(f"[AUDIO] Dialog-script byggt fr√•n {len(dialog_lines)} parsade repliker")
        else:
            # Skapa dialog-script fr√•n inneh√•ll (fallback)
            dialog_script = generator.create_dialog_script(script_content, weather_info)
            logger.info("[AUDIO] Dialog-script skapat f√∂r Lisa och Pelle")

        if not dialog_script:
            logger.warning("[AUDIO] Tomt dialog-script f√∂r Gemini, faller tillbaka")
            return False
        
        # Generera audio med freeform dialog
        success = generator.synthesize_dialog_freeform(
            dialog_script=dialog_script,
            output_file=output_file
        )
        
        if success:
            logger.info(f"[AUDIO] ‚úÖ Gemini TTS dialog sparad: {output_file}")
            _set_last_gemini_error(None)
            return True
        else:
            logger.warning("[AUDIO] Gemini TTS misslyckades, faller tillbaka till standard")
            _set_last_gemini_error("synthesize_dialog_freeform returned False (see [GEMINI-TTS] logs)")
            return False
            
    except Exception as e:
        _set_last_gemini_error(f"{type(e).__name__}: {e}")
        logger.exception("[AUDIO] Gemini TTS fel")
        logger.info("[AUDIO] Faller tillbaka till standard TTS")
        return False

def generate_audio_google_cloud(segments: List[Dict], output_file: str) -> bool:
    """Generera audio med Google Cloud TTS (fallback-metod)"""
    try:
        # Anv√§nd r√§tt TTS-klass
        from google_cloud_tts import GoogleCloudTTS
        
        # Konvertera segments till r√§tt format
        audio_segments = []
        for i, segment in enumerate(segments):
            speaker = segment['speaker'].lower()
            
            # Mappa talare till r√∂ster
            if 'lisa' in speaker:
                voice = 'lisa'
            elif 'pelle' in speaker:
                voice = 'pelle'
            else:
                # Alternerande r√∂ster
                voice = 'lisa' if i % 2 == 0 else 'pelle'
            
            audio_segments.append({
                'voice': voice,
                'text': segment['text']
            })
        
        # Skapa TTS-instans och generera audio
        tts = GoogleCloudTTS()
        audio_file = tts.generate_podcast_audio(audio_segments)
        
        if audio_file and os.path.exists(audio_file):
            shutil.move(audio_file, output_file)
            logger.info(f"[TTS] Audio genererad: {output_file}")
            return True
        else:
            logger.error("[ERROR] TTS-generering misslyckades")
            return False
            
    except Exception as e:
        logger.error(f"[ERROR] TTS-fel: {e}")
        return False

def add_music_to_podcast(audio_file: str) -> str:
    """L√§gg till musik och bryggkor till podcast med MusicMixer"""
    try:
        logger.info("[MUSIC] L√§gger till musik och bryggkor...")
        
        # Skapa en MusicMixer-instans
        mixer = MusicMixer()
        
        # Skanna alla mp3-filer i music-mappen automatiskt
        music_dir = "audio/music"
        available_music = []
        
        if os.path.exists(music_dir):
            for filename in os.listdir(music_dir):
                if filename.lower().endswith('.mp3'):
                    full_path = os.path.join(music_dir, filename)
                    available_music.append(full_path)
        
        logger.info(f"[MUSIC] Hittade {len(available_music)} musikfiler: {[os.path.basename(f) for f in available_music]}")
        
        if not available_music:
            logger.warning("[MUSIC] Inga musikfiler hittades")
            return audio_file
            
        # Skapa varierad musikmix f√∂r hela avsnittet
        import random
        music_output = audio_file.replace('.mp3', '_with_music.mp3')
        
        # Nya f√∂rb√§ttrade musikmixning med variation
        success = mixer.create_varied_music_background(
            speech_file=audio_file,
            available_music=available_music,
            output_file=music_output,
            music_volume=-15,  # L√•g volym i dB
            segment_duration=60,  # Byt musik var 60:e sekund
            fade_duration=3000    # 3 sekunder crossfade mellan l√•tar
        )
        
        if success and os.path.exists(music_output):
            # Ers√§tt original med musikversion
            shutil.move(music_output, audio_file)
            logger.info(f"[MUSIC] Musik tillagd till {audio_file}")
            return audio_file
        else:
            logger.warning("[MUSIC] Kunde inte l√§gga till musik, anv√§nder original")
            return audio_file
            
    except Exception as e:
        logger.error(f"[MUSIC] Fel vid musiktill√§gg: {e}")
        return audio_file  # Returnera original om musik misslyckas


def enforce_intro_date(script_text: str, weekday: str, day: int, month: str) -> str:
    """Replace incorrect intro date phrases with the actual Swedish date."""
    if not script_text:
        return script_text

    def _replace_intro(match: re.Match) -> str:
        intro_line = match.group(0)
        comma_index = intro_line.find(',')
        suffix = intro_line[comma_index:] if comma_index != -1 else ''
        logger.info("[SCRIPT] Uppdaterar intro-datum till dagens datum")
        return f"Idag √§r det {weekday} den {day} {month}{suffix}"

    return re.sub(r"Idag √§r det[^\n]*", _replace_intro, script_text, count=1)


def clean_xml_text(text: Optional[str]) -> str:
    """Sanitize text for safe XML output"""
    if not text:
        return ""

    cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', str(text))
    normalized = html.unescape(cleaned)
    return html.escape(normalized, quote=True)


def generate_github_rss(episodes_data: List[Dict], base_url: str) -> str:
    """Generera RSS-feed.

    Metadata f√∂r kanalen h√§mtas fr√•n sources.json (podcastSettings) f√∂r att
    √§ndringar i "Podcast Settings" ska sl√• igenom i produktion.
    """
    config = load_config() or {}
    ps = (config.get('podcastSettings') or {}) if isinstance(config.get('podcastSettings'), dict) else {}

    channel_title = (ps.get('title') or "MMM Senaste Nytt").strip()
    channel_description = (ps.get('description') or "").strip() or (
        "Dagliga nyheter fr√•n v√§rlden av m√§nniska, maskin och milj√∂ - med Lisa och Pelle. "
        "En del av M√§nniska Maskin Milj√∂-familjen."
    )
    itunes_author = (ps.get('author') or "").strip() or "Pontus Dahlberg"
    itunes_explicit = bool(ps.get('explicit', False))
    itunes_category = (ps.get('category') or "Technology").strip()
    itunes_email = os.getenv('PODCAST_EMAIL', '').strip() or "podcast@example.com"
    rss_items = []
    
    for episode in episodes_data:
        # Hantera b√•da gamla (date) och nya (pub_date) format
        if 'pub_date' in episode:
            pub_date = episode['pub_date']  # Redan i r√§tt format
        elif 'date' in episode:
            pub_date = datetime.strptime(episode['date'], '%Y-%m-%d').strftime('%a, %d %b %Y %H:%M:%S +0000')
        else:
            pub_date = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S +0000')
        
        # Hantera b√•de gamla och nya format f√∂r audio URL och storlek
        if 'audio_url' in episode:
            # Nytt format fr√•n episodhistorik
            audio_url = episode['audio_url']
            file_size = episode.get('file_size', 7000000)
            guid = episode.get('guid', audio_url)
        else:
            # Gammalt format fr√•n direct generation
            audio_url = f"{base_url}/audio/{episode['filename']}"
            file_size = episode.get('size', 7000000)
            guid = audio_url
        
        safe_title = clean_xml_text(episode.get('title', ''))
        safe_description = clean_xml_text(episode.get('description', ''))
        safe_audio_url = html.escape(audio_url, quote=True)
        safe_guid = clean_xml_text(guid)

        rss_items.append(f"""        <item>
            <title>{safe_title}</title>
            <description>{safe_description}</description>
            <pubDate>{pub_date}</pubDate>
            <enclosure url="{safe_audio_url}" length="{file_size}" type="audio/mpeg"/>
            <guid>{safe_guid}</guid>
        </item>""")
    
    rss_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">
    <channel>
        <title>{clean_xml_text(channel_title)}</title>
        <description>{clean_xml_text(channel_description)}</description>
        <link>{base_url}</link>
        <language>sv-SE</language>
        <itunes:category text="{clean_xml_text(itunes_category)}"/>
        <itunes:explicit>{'true' if itunes_explicit else 'false'}</itunes:explicit>
        <itunes:author>{clean_xml_text(itunes_author)}</itunes:author>
        <itunes:summary>{clean_xml_text(channel_description)}</itunes:summary>
        <itunes:owner>
            <itunes:name>{clean_xml_text(itunes_author)}</itunes:name>
            <itunes:email>{clean_xml_text(itunes_email)}</itunes:email>
        </itunes:owner>
        <itunes:image href="{base_url}/cover.jpg"/>
        <lastBuildDate>{datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')}</lastBuildDate>
        {''.join(rss_items)}
    </channel>
</rss>"""
    
    return rss_content

def main():
    """Huvudfunktion f√∂r komplett podcast-generering med musik och v√§der"""
    logger.info("[PODCAST] Startar MMM Senaste Nytt med musik och v√§der...")
    
    # Log Gemini TTS status after logger is initialized
    if GEMINI_TTS_AVAILABLE:
        logger.info("[SYSTEM] Gemini TTS tillg√§nglig f√∂r naturlig dialog")
    else:
        logger.warning("[SYSTEM] Gemini TTS inte tillg√§nglig - anv√§nder standard TTS")

    log_diagnostic('system_tts_capabilities', {
        'gemini_tts_available': bool(GEMINI_TTS_AVAILABLE),
        'gemini_tts_import_error': GEMINI_TTS_IMPORT_ERROR,
    })
    
    # Log Fact Checker status
    if FACT_CHECKER_AVAILABLE:
        logger.info("[SYSTEM] üõ°Ô∏è KRITISK faktakontroll-agent aktiverad f√∂r s√§kerhet")
    else:
        logger.error("[SYSTEM] ‚ö†Ô∏è VARNING: Faktakontroll-agent inte tillg√§nglig - RISK F√ñR FELAKTIG INFO!")
    
    try:
        # H√§mta v√§derdata f√∂rst
        logger.info("[WEATHER] H√§mtar aktuell v√§derdata...")
        weather_info = get_swedish_weather()
        logger.info(f"[WEATHER] {weather_info}")
        
        # Ladda konfiguration
        config = load_config()
        
        # Generera datum och filnamn
        today = datetime.now()
        timestamp = today.strftime('%Y%m%d_%H%M%S')

        # S√§tt run-id s√• att diagnostics kan korreleras mellan moduler
        set_run_id(timestamp)
        os.environ['MMM_RUN_ID'] = timestamp
        
        # Skapa output-mappar
        os.makedirs('audio', exist_ok=True)
        os.makedirs('public/audio', exist_ok=True)
        
        # Generera strukturerat podcast-inneh√•ll med riktig v√§derdata
        logger.info("[AI] Genererar strukturerat podcast-inneh√•ll...")
        podcast_content, referenced_articles = generate_structured_podcast_content(weather_info, today=today)

        weekday_swedish = SWEDISH_WEEKDAYS.get(today.strftime('%A'), today.strftime('%A'))
        month_swedish = SWEDISH_MONTHS.get(today.month, today.strftime('%B').lower())
        podcast_content = enforce_intro_date(podcast_content, weekday_swedish, today.day, month_swedish)
        
        # Spara manus f√∂r referens
        script_path = f"podcast_script_{timestamp}.txt"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(podcast_content)
        logger.info(f"[SCRIPT] Manus sparat: {script_path}")
        
        # Spara artikelreferenser f√∂r senare anv√§ndning
        articles_path = f"episode_articles_{timestamp}.json"
        with open(articles_path, 'w', encoding='utf-8') as f:
            json.dump(referenced_articles, f, indent=2, ensure_ascii=False)
        logger.info(f"[ARTICLES] Artikelreferenser sparade: {articles_path}")
        
        # üõ°Ô∏è SJ√ÑLVKORRIGERANDE FAKTAKONTROLL - Automatisk korrigering av problem
        final_podcast_content = podcast_content
        max_correction_attempts = 3

        # Sammanfattning som kan anv√§ndas i kvalitetsrapport
        fact_check_summary = {
            'status': 'UNKNOWN',
            'warnings': [],
            'critical_issues_count': None,
            'correction_attempts': 0,
            'auto_correct_used': False,
        }
        
        for correction_attempt in range(max_correction_attempts):
            logger.info(f"[FACT-CHECK] üõ°Ô∏è Faktakontroll f√∂rs√∂k {correction_attempt + 1}/{max_correction_attempts}")
            fact_check_summary['correction_attempts'] = correction_attempt + 1
            
            # Grundl√§ggande faktakontroll f√∂rst (snabbast)
            fact_check_passed = False
            if BASIC_FACT_CHECKER_AVAILABLE:
                basic_checker = BasicFactChecker()
                basic_result = basic_checker.basic_fact_check(final_podcast_content)
                
                if basic_result['safe_to_publish']:
                    # Visa varningar men godk√§nn √§nd√•
                    warnings = basic_result.get('warnings', [])
                    fact_check_summary['warnings'] = warnings
                    fact_check_summary['critical_issues_count'] = 0
                    if warnings:
                        logger.info(f"[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd med varningar: {warnings}")
                    else:
                        logger.info("[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd helt")
                    fact_check_summary['status'] = 'SAFE'
                    fact_check_passed = True
                    break
                else:
                    critical_issues = basic_result.get('critical_issues', [])
                    warnings = basic_result.get('warnings', [])
                    fact_check_summary['warnings'] = warnings
                    fact_check_summary['critical_issues_count'] = len(critical_issues)
                    fact_check_summary['status'] = 'REQUIRES_REVIEW'
                    logger.error(f"[FACT-CHECK] ‚ùå Kritiska problem hittade: {critical_issues}")
                    if warnings:
                        logger.info(f"[FACT-CHECK] ‚ÑπÔ∏è Varningar (blockerar inte): {warnings}")
                    
                    # F√∂rs√∂k automatisk korrigering
                    if SELF_CORRECTING_AVAILABLE and correction_attempt < max_correction_attempts - 1:
                        logger.info("[FACT-CHECK] üîß Startar automatisk korrigering...")
                        
                        corrected_content, correction_success = auto_correct_podcast_content(
                            final_podcast_content, basic_result.get('critical_issues', [])
                        )
                        
                        if correction_success:
                            logger.info("[FACT-CHECK] ‚úÖ Automatisk korrigering lyckades!")
                            final_podcast_content = corrected_content
                            fact_check_summary['auto_correct_used'] = True
                            
                            # Spara korrigerat manus
                            corrected_script_path = f"podcast_script_{timestamp}_corrected_v{correction_attempt + 1}.txt"
                            with open(corrected_script_path, 'w', encoding='utf-8') as f:
                                f.write(final_podcast_content)
                            logger.info(f"[SCRIPT] Korrigerat manus sparat: {corrected_script_path}")
                            continue
                        else:
                            logger.warning("[FACT-CHECK] ‚ö†Ô∏è Automatisk korrigering misslyckades")
                    else:
                        logger.warning("[FACT-CHECK] ‚ö†Ô∏è Sj√§lvkorrigering inte tillg√§nglig")
            
            # Om vi n√•r hit har korrigering misslyckats eller √§r sista f√∂rs√∂ket
            break
        
        # Final kontroll
        if not fact_check_passed:
            logger.error("üö® PUBLICERING STOPPAD - FAKTAKONTROLL MISSLYCKADES!")
            fact_check_summary['status'] = 'FAILED'
            
            # Spara rapport f√∂r manuell granskning
            final_report_path = f"fact_check_failed_{timestamp}.txt"
            with open(final_report_path, 'w', encoding='utf-8') as f:
                f.write("FAKTAKONTROLL MISSLYCKADES\n")
                f.write(f"Datum: {datetime.now().isoformat()}\n")
                f.write(f"F√∂rs√∂k gjorda: {correction_attempt + 1}\n\n")
                if 'basic_result' in locals():
                    f.write("SENASTE PROBLEM:\n")
                    for issue in basic_result['issues_found']:
                        f.write(f"- {issue}\n")
            
            print(f"\nüö® S√ÑKERHETSVARNING: Automatisk korrigering misslyckades!")
            print(f"Rapport sparad: {final_report_path}")
            print("Manuell granskning kr√§vs. Se MANUAL_FACT_CHECK_GUIDE.md")
            return False
        else:
            logger.info("[FACT-CHECK] ‚úÖ Faktakontroll godk√§nd - s√§kert att publicera")
            # Uppdatera inneh√•llet om det korrigerades
            if final_podcast_content != podcast_content:
                logger.info("[FACT-CHECK] üìù Anv√§nder automatiskt korrigerat inneh√•ll")
                podcast_content = final_podcast_content
        
        # Parsa inneh√•llet i segment
        segments = parse_podcast_text(podcast_content)
        logger.info(f"[PARSE] Hittade {len(segments)} segment att generera audio f√∂r")
        
        # R√§kna ord f√∂r att uppskatta l√§ngd
        total_words = sum(len(segment['text'].split()) for segment in segments)
        estimated_minutes = total_words / 150  # Ungef√§r 150 ord per minut i tal
        logger.info(f"[ESTIMATE] {total_words} ord, uppskattad l√§ngd: {estimated_minutes:.1f} minuter")
        
        # Generera filnamn
        audio_filename = f"MMM_senaste_nytt_{timestamp}.mp3"
        audio_filepath = os.path.join('audio', audio_filename)

        require_gemini_tts = os.getenv('MMM_FORCE_GEMINI_TTS', '').strip().lower() in {'1', 'true', 'yes', 'y'}
        
        # F√∂rs√∂k f√∂rst med Gemini TTS f√∂r naturlig dialog
        logger.info("[TTS] F√∂rs√∂ker generera naturlig dialog med Gemini TTS...")
        log_diagnostic('tts_provider_attempt', {
            'provider': 'gemini',
            'output_file': audio_filepath,
            'require_gemini': require_gemini_tts,
        })
        gemini_success = generate_audio_with_gemini_dialog(podcast_content, weather_info, audio_filepath)
        
        if not gemini_success:
            log_diagnostic('tts_provider_result', {
                'provider': 'gemini',
                'success': False,
                'error': _LAST_GEMINI_ERROR,
            })

            if require_gemini_tts:
                logger.error("[TTS] MMM_FORCE_GEMINI_TTS √§r aktivt: avbryter ist√§llet f√∂r fallback")
                return False

            # Fallback till standard Google Cloud TTS
            logger.info("[TTS] Anv√§nder standard Google Cloud TTS som fallback...")
            log_diagnostic('tts_provider_fallback', {
                'from_provider': 'gemini',
                'to_provider': 'google_cloud',
            })
            success = generate_audio_google_cloud(segments, audio_filepath)

            log_diagnostic('tts_provider_result', {
                'provider': 'google_cloud',
                'success': bool(success),
            })
            
            if not success:
                logger.error("[ERROR] Audio-generering misslyckades")
                return False
        else:
            logger.info("[TTS] ‚úÖ Naturlig dialog genererad med Gemini TTS!")
            log_diagnostic('tts_provider_result', {
                'provider': 'gemini',
                'success': True,
            })
        
        # L√§gg till musik och bryggkor
        audio_filepath = add_music_to_podcast(audio_filepath)
        
        # Kopiera till public/audio f√∂r GitHub Pages
        public_audio_path = os.path.join('public', 'audio', audio_filename)
        shutil.copy2(audio_filepath, public_audio_path)
        logger.info(f"[FILES] Kopierade audio till {public_audio_path}")
        
        # Skapa episode data med artiklar som faktiskt refererades i avsnittet
        file_size = os.path.getsize(audio_filepath)
        
        # F√∂r RSS: lista bara k√§llor som sannolikt faktiskt n√§mns i manuset
        logger.info(f"[RSS] Antal kandidat-artiklar: {len(referenced_articles)}")

        rss_referenced_articles = extract_referenced_articles(podcast_content, referenced_articles, max_results=6)
        logger.info(f"[RSS] Antal matchade artiklar i manus: {len(rss_referenced_articles)}")

        if not rss_referenced_articles and referenced_articles:
            logger.warning("[RSS] 0 matchade artiklar i manus; faller tillbaka till topp-kandidater f√∂r k√§llor i beskrivningen")
            log_diagnostic('rss_sources_fallback_used', {
                'candidate_count': len(referenced_articles),
                'fallback_count': min(6, len(referenced_articles)),
            })
            rss_referenced_articles = referenced_articles[:6]

        # Diagnostics: visa vilka som inte matchades (hj√§lper fels√∂ka "k√§llor som inte var med")
        try:
            matched_keys = set()
            for a in rss_referenced_articles:
                key = _canonicalize_url(a.get('link', '')) or _title_fingerprint(a.get('title', ''))
                if key:
                    matched_keys.add(key)

            unmatched = []
            for a in referenced_articles:
                key = _canonicalize_url(a.get('link', '')) or _title_fingerprint(a.get('title', ''))
                if key and key not in matched_keys:
                    unmatched.append({'source': a.get('source', ''), 'title': a.get('title', ''), 'link': a.get('link', '')})

            if unmatched:
                log_diagnostic('rss_sources_unmatched', {
                    'candidate_count': len(referenced_articles),
                    'matched_count': len(rss_referenced_articles),
                    'unmatched_count': len(unmatched),
                    'examples': unmatched[:10],
                })
        except Exception:
            pass

        article_links = []
        for i, article in enumerate(rss_referenced_articles):
            logger.info(f"[RSS] Artikel {i+1}: {article.get('source', 'N/A')} - {article.get('title', 'N/A')[:50]}...")
            if article.get('link') and article.get('title'):
                short_title = article['title'][:60] + "..." if len(article['title']) > 60 else article['title']
                source_name = article.get('source', 'Ok√§nd k√§lla')
                article_links.append(f"{source_name}: {short_title}\n  {article['link']}")
        
        sources_text = ""
        if article_links:
            sources_text = f"\n\nK√§llor som refereras i detta avsnitt:\n‚Ä¢ " + "\n‚Ä¢ ".join(article_links)
            logger.info(f"[RSS] Lade till {len(article_links)} k√§llor i RSS-beskrivning")
        else:
            logger.warning("[RSS] Inga k√§llor att visa i RSS-beskrivning!")
        
        month_swedish = SWEDISH_MONTHS[today.month]
        weekday_swedish = SWEDISH_WEEKDAYS.get(today.strftime('%A'), today.strftime('%A'))
        
        episode_data = {
            'title': f"MMM Senaste Nytt - {today.day} {month_swedish} {today.year}",
            'description': f"Dagens nyheter inom AI, teknik och klimat - {weekday_swedish} den {today.day} {month_swedish} {today.year}. Med detaljerade k√§llh√§nvisningar fr√•n svenska och internationella medier.{sources_text}",
            'date': today.strftime('%Y-%m-%d'),
            'filename': audio_filename,
            'size': file_size,
            'duration': f"{estimated_minutes:.0f}:00"
        }
        
        # L√§gg till episod till historik och generera RSS-feed med alla episoder
        logger.info("[HISTORY] L√§gger till episod till historik...")
        history = EpisodeHistory()
        all_episodes = history.add_episode(episode_data)
        
        # Generera RSS-feed med alla episoder (max 10 senaste f√∂r RSS-prestanda)
        base_url = "https://pontusdahlberg.github.io/morgonpodden"
        recent_episodes = all_episodes[:10]  # Ta bara 10 senaste f√∂r RSS-feed
        rss_content = generate_github_rss(recent_episodes, base_url)
        
        # Spara RSS-feed
        rss_path = os.path.join('public', 'feed.xml')
        with open(rss_path, 'w', encoding='utf-8') as f:
            f.write(rss_content)
        logger.info(f"[RSS] RSS-feed sparad med {len(recent_episodes)} episoder: {rss_path}")

        # ============================================================
        # KVALITETSRAPPORT (relevans/korrekthet/k√∂rningsproblem)
        # ============================================================
        try:
            from src.episode_quality import generate_episode_quality_report, write_quality_reports

            scraped_data_for_report = None
            try:
                with open('scraped_content.json', 'r', encoding='utf-8') as f:
                    scraped_data_for_report = json.load(f)
            except Exception:
                scraped_data_for_report = None

            quality_report = generate_episode_quality_report(
                run_id=timestamp,
                script_text=podcast_content,
                referenced_articles=referenced_articles,
                scraped_content=scraped_data_for_report,
                diagnostics_file=DIAGNOSTICS_FILE,
                fact_check_summary=fact_check_summary,
            )
            paths = write_quality_reports(report=quality_report, output_dir='episodes')
            logger.info(f"[QUALITY] Rapport sparad: {paths.get('markdown')} | {paths.get('json')}")

            # Valfritt: mejla rapporten om SMTP/MMM_REPORT_EMAIL_* √§r konfigurerat
            try:
                from src.report_emailer import maybe_email_quality_report

                diag_max_env = os.getenv('MMM_EMAIL_DIAG_MAX_LINES', '').strip()
                log_tail_env = os.getenv('MMM_EMAIL_LOG_TAIL_LINES', '').strip()
                try:
                    diag_max_lines = int(diag_max_env) if diag_max_env else 2000
                except ValueError:
                    diag_max_lines = 2000
                try:
                    log_tail_lines = int(log_tail_env) if log_tail_env else 600
                except ValueError:
                    log_tail_lines = 600

                diag_extract = _write_run_diagnostics_extract(
                    run_id=timestamp,
                    diagnostics_file=DIAGNOSTICS_FILE,
                    output_dir='episodes',
                    max_lines=max(200, diag_max_lines),
                )
                log_tail = _write_log_tail(
                    run_id=timestamp,
                    log_path='podcast_generation.log',
                    output_dir='episodes',
                    max_lines=max(200, log_tail_lines),
                )

                extra_attachments = [p for p in [diag_extract, log_tail] if p]

                emailed = maybe_email_quality_report(
                    run_id=timestamp,
                    markdown_path=paths.get('markdown'),
                    json_path=paths.get('json'),
                    extra_attachments=extra_attachments,
                )
                if emailed:
                    logger.info("[QUALITY] ‚úÖ Kvalitetsrapport mejlad")
            except Exception as e:
                logger.warning(f"[QUALITY] Kunde inte mejla kvalitetsrapport: {e}")
        except Exception as e:
            logger.warning(f"[QUALITY] Kunde inte skapa kvalitetsrapport: {e}")
        
        # Logga framg√•ng
        logger.info("[SUCCESS] Komplett podcast-generering slutf√∂rd!")
        logger.info(f"[AUDIO] Audio: {audio_filepath}")
        logger.info(f"[SCRIPT] Manus: {script_path}")
        logger.info(f"[RSS] RSS: {rss_path}")
        logger.info(f"[WEATHER] V√§der: {weather_info}")
        logger.info(f"[STATS] Segment: {len(segments)}, Ord: {total_words}, L√§ngd: ~{estimated_minutes:.1f} min")
        logger.info(f"[GITHUB] GitHub Pages URL: {base_url}")
        logger.info(f"[FEED] RSS URL: {base_url}/feed.xml")
        
        return True
        
    except Exception as e:
        logger.error(f"[ERROR] Ov√§ntat fel: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == '__main__':
        if any(arg in {'-h', '--help'} for arg in sys.argv[1:]):
                print(
                        """Usage:
    python run_podcast_complete.py

Genererar MMM Senaste Nytt (nyheter + v√§der + TTS + musik + RSS).

Valfria env vars (fels√∂kning):
    MMM_FORCE_GEMINI_TTS=1
        - Avbryter k√∂rningen om Gemini-TTS misslyckas (ingen tyst fallback).

    GEMINI_TTS_PROMPT_MAX_BYTES=850
        - Maxstorlek f√∂r prompt (UTF-8 bytes).

    GEMINI_TTS_MAX_BYTES=3900
        - Maxstorlek per chunk i TTS-input (UTF-8 bytes).
"""
                )
                sys.exit(0)

        success = main()
        sys.exit(0 if success else 1)